{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7faaa133-7ba0-4b01-bd63-0670b1365cac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks Setup\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, sin, randn, expr\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "\n",
    "# Storage Location in DBFS\n",
    "DELTA_TABLE_PATH = \"/mnt/oil_data/oil_yield_delta\"\n",
    "\n",
    "# Parameters\n",
    "NUM_WELLS = 10\n",
    "DAYS_TO_ADD = 30\n",
    "\n",
    "# Initialize Spark\n",
    "spark = SparkSession.builder.appName(\"OilYieldIngestion\").getOrCreate()\n",
    "\n",
    "# Generate Synthetic Data\n",
    "def generate_synthetic_data(num_wells, start_date, days):\n",
    "    wells = [f\"WELL_{i+1:03d}\" for i in range(num_wells)]\n",
    "    dates = [start_date + timedelta(days=i) for i in range(days)]\n",
    "\n",
    "    # Generate DataFrame\n",
    "    data = []\n",
    "    for well_id in wells:\n",
    "        base_yield = np.random.uniform(100, 500)\n",
    "        for date in dates:\n",
    "            seasonal_variation = 20 * np.sin(2 * np.pi * (date.timetuple().tm_yday) / 365)\n",
    "            noise = np.random.normal(0, 10)\n",
    "            daily_yield = max(50, base_yield + seasonal_variation + noise)\n",
    "            data.append((well_id, date, round(daily_yield, 2)))\n",
    "\n",
    "    return spark.createDataFrame(data, [\"well_id\", \"date\", \"yield_bbl\"])\n",
    "\n",
    "# Load Existing Data from Delta\n",
    "if spark.catalog._jcatalog.tableExists(\"oil_forecasting.oil_yield\"):\n",
    "    existing_df = spark.read.format(\"delta\").load(DELTA_TABLE_PATH)\n",
    "    last_date = existing_df.selectExpr(\"max(date)\").collect()[0][0]\n",
    "    new_start_date = last_date + timedelta(days=1)\n",
    "    print(f\"Appending data from {new_start_date} onward...\")\n",
    "else:\n",
    "    new_start_date = datetime(2023, 1, 1)\n",
    "\n",
    "# Generate & Append New Data\n",
    "new_df = generate_synthetic_data(NUM_WELLS, new_start_date, DAYS_TO_ADD)\n",
    "new_df.write.format(\"delta\").mode(\"append\").save(DELTA_TABLE_PATH)\n",
    "\n",
    "print(f\"Added {new_df.count()} new records to Delta Lake.\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "00_generate_data",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
