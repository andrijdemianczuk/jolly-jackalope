{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "754dfed1-1df4-445b-a052-ee9c076f66e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Oil Extraction Production Forecasting\n",
    "<br/>\n",
    "<img src=\"https://www.nsenergybusiness.com/wp-content/uploads/sites/4/2022/07/refinery-ga56d4972f_640.jpg\" />\n",
    "\n",
    "## Building our model using MLFlow\n",
    "Training and tuning a model using MLflow in Databricks provides a structured and efficient way to manage experiments, track performance, and optimize hyperparameters. With MLflow Tracking, you can log metrics, parameters, and artifacts, allowing you to compare different runs systematically. When tuning a model, you can integrate Hyperopt, Optuna, or custom grid/random search within MLflow to automate hyperparameter optimization. Once the best model is identified, MLflow Models enables seamless registration, versioning, and deployment, ensuring that the optimized model can be easily used for inference in production environments. This approach enhances reproducibility, collaboration, and operationalization of ML workflows.\n",
    "\n",
    "In this notebook we'll be taking a look at how to construct an experiment to ensure that the best and most reliable models are compiled, logged, and registered for production inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "da958da9-76a5-4698-8458-d183e4b777d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Loading custom libraries\n",
    "For this notebook, we'll need to load some custom libraries. Specifically we'll be requiring joblib and joblibspark for distributed tuning. I've also included hyperopt as an alternative to Optuna. Using the `%pip` install command ensures all nodes in the cluster receive the package install.\n",
    "\n",
    "_Note:_ After installing python packages across the cluster it's recommended to restart the python interpreter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d787d8b-b41b-4af2-bfc9-407aa8cd77ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install hyperopt joblib pyspark joblibspark\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6435c9d1-456c-44db-a945-5aa3bb78ffad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Initialization\n",
    "Below is an initialization block to help us out. This is designed so that each user has their own set of unique names credentials. Don't worry too much about what it's doing - this is mostly because we have several users doing the same lab with the same parameters in a shared workspace and don't want any collisions. For enterprise work this is largely unnecessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7bf4a4b0-2add-47da-b679-529edfd7edd7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import hashlib, base64\n",
    "\n",
    "#IMPORTANT! DO NOT CHANGE THESE VALUES!!!!\n",
    "catalog = \"workshop\"\n",
    "db = \"default\"\n",
    "current_user = dbutils.notebook.entry_point.getDbutils().notebook().getContext().tags().get(\"user\").get()\n",
    "hash_object = hashlib.sha256(current_user.encode())\n",
    "hash_user_id = base64.b32encode(hash_object.digest()).decode(\"utf-8\").rstrip(\"=\")[:12]  #Trim to 12 chars for readability\n",
    "initials = \"\".join([x[0] for x in current_user.split(\"@\")[0].split(\".\")])\n",
    "short_hash = hashlib.md5(current_user.encode()).hexdigest()[:8]  #Short 8-char hash\n",
    "safe_user_id = f\"{initials.upper()}_{short_hash}\"\n",
    "src_table = f\"{safe_user_id}_oil_yield\"\n",
    "model_name = f\"{safe_user_id}_oil_yield_forecast\"\n",
    "model_uri = f\"{catalog}.{db}.{model_name}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "38833416-0b3f-4ac0-821f-aaaa54350ed5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Setting our experiment and load our features\n",
    "Just like before, we'll use the common experiment and read our latest feature table with our transformations applied using the feature engineering client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7eb48a0c-3ad7-4aba-a74e-e9deb1e75c7b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "# Set a named experiment\n",
    "mlflow.set_experiment(f\"/Users/{current_user}/Oil Extraction Production Forecasting\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b68dbb96-527b-428d-b07b-4b82f8eb63da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks.feature_engineering import FeatureEngineeringClient\n",
    "\n",
    "fe = FeatureEngineeringClient()\n",
    "\n",
    "df = fe.read_table(\n",
    "  name=f'{catalog}.{db}.{src_table}_features_transformed'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "24142f8e-c21e-45c0-81eb-d3ead10220d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Training and hyperparameter tuning\n",
    "This is one of the most important parts of the experiment. Now that we've identified our features and performed the necessary transformations it's time to start building our trial runs to find the best way to tune a model. Most training libraries have ways we can tune our algorithms to further improve performance. These meta parameters are called hyperparameters and they affect how the training pipeline behaves. Since we're using MLFlow, we can log each of these runs and see how adjusting parameters in a variety of ways can impact model performance.\n",
    "\n",
    "**From Google:**\n",
    "Hyperparameter tuning is the process of optimizing the configuration settings that control how a machine learning model learns, rather than the parameters it learns from data. These hyperparameters impact model performance, complexity, and generalization.\n",
    "\n",
    "In XGBoost, hyperparameter tuning is crucial for maximizing predictive accuracy and avoiding overfitting. Key hyperparameters include:\n",
    "- \tLearning rate (eta) – Controls step size during training.\n",
    "- \tMax depth – Limits tree depth to balance bias-variance tradeoff.\n",
    "- \tMin child weight – Prevents overly complex trees.\n",
    "- \tSubsample & colsample_bytree – Improve generalization by using random subsets of data and features.\n",
    "- \tGamma & lambda – Regulate tree pruning and L1/L2 regularization.\n",
    "\n",
    "Defining the search space is essential to ensure tuning focuses on meaningful hyperparameter ranges rather than blindly testing values. Grid search, random search, and Bayesian optimization (Hyperopt) refine the selection process, balancing computational efficiency with finding the best model configuration. Properly tuning XGBoost leads to higher accuracy, better generalization, and reduced overfitting, making it a key step in model optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "65f6b51d-f1d7-4d13-a997-a732e4a8f145",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### A quick note on algorithms\n",
    "##### XGBoost\n",
    "XGBoost, which stands for Extreme Gradient Boosting, is a type of gradient boosting algorithm that uses decision trees as its base learners, making it an ensemble learning method where multiple decision trees are combined to make predictions with higher accuracy; it is considered a powerful and scalable machine learning algorithm often used for both classification and regression tasks.\n",
    "\n",
    "A Gradient Boosted Regressor in XGBoost is a powerful machine learning algorithm that builds an ensemble of decision trees sequentially, where each new tree corrects the errors of the previous ones. It uses gradient descent to minimize the loss function, making it highly effective for forecasting tasks where capturing complex, nonlinear relationships is crucial.\n",
    "\n",
    "In the context of forecasting, XGBoost’s gradient boosting approach helps:\n",
    "- Capture trends and interactions in time-series and structured data.\n",
    "- Handle missing values and outliers more robustly than traditional regressors.\n",
    "- Improve predictive accuracy by reducing bias and variance through iterative learning.\n",
    "\n",
    "With hyperparameter tuning and feature engineering, XGBoost can be a highly efficient choice for forecasting oil yield, energy prices, or demand trends, leveraging both structured time-series features and external variables for improved performance.\n",
    "\n",
    "##### Prophet\n",
    "Prophet is a time series forecasting algorithm based on an additive regression model, which means it decomposes a time series into components like trend, seasonality, and holidays to make predictions, making it particularly useful for data with strong seasonal patterns and non-linear trends; it's considered a type of machine learning algorithm when applied to data to generate forecasts. Prophet comes from Meta (the Facebook guys) and is a fairly lightweight, easy to use algorithm. We will see it later when we use it to do a short-term prediction of our boosting features.\n",
    "##### LSTM\n",
    "LSTM stands for \"Long Short-Term Memory\" and is a type of Recurrent Neural Network (RNN) algorithm, specifically designed to handle long-term dependencies in sequential data by overcoming the vanishing gradient problem common to traditional RNNs; making it effective for tasks like time series prediction and natural language processing. \n",
    "Key points about LSTM:\n",
    "Special architecture:\n",
    "LSTMs use specialized \"gates\" to control the flow of information through the network, allowing them to selectively remember relevant information from the past while forgetting irrelevant details. \n",
    "Applications:\n",
    "Used in tasks like language modeling, machine translation, speech recognition, time series forecasting, and handwriting recognition. \n",
    "Advantage over standard RNNs:\n",
    "LSTM's ability to learn long-term dependencies makes it superior to standard RNNs which often struggle with remembering information from distant past inputs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2ef593f6-8f79-4750-87ec-12f1c4372e8d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Parallelizing our hyperparameter tuning\n",
    "Using Joblib-Spark on Databricks enables efficient parallelization of hyperparameter tuning with Optuna by distributing trial evaluations across a Spark cluster. Since Optuna is inherently single-threaded, integrating it with Joblib-Spark allows multiple trials to run concurrently on different nodes, significantly speeding up the search for optimal hyperparameters.\n",
    "\n",
    "By setting optuna.integration.JoblibStudy with joblibspark.register_spark(), you can leverage Spark’s distributed computing to scale Bayesian optimization, reducing tuning time while maintaining model performance. This approach is particularly useful for training compute-intensive models like XGBoost, ensuring faster convergence and better utilization of Databricks’ cluster resources.\n",
    "\n",
    "##### joblibspark() v. joblib with Loky\n",
    "\n",
    "Both joblibspark and loky (joblib.parallel_backend(\"loky\", n_jobs=-1)) enable parallelization for hyperparameter tuning with Optuna, but they differ in how they distribute computations:\n",
    "\n",
    "1. Joblib-Spark (joblibspark.register_spark())\n",
    "- Best for distributed environments (Databricks, Spark clusters)\n",
    "- Runs Optuna trials across multiple Spark workers, allowing efficient use of a Databricks cluster.\n",
    "- Scales well for large datasets and compute-intensive models (e.g., XGBoost, deep learning).\n",
    "- Reduces the memory overhead on a single machine by distributing the workload.\n",
    "- Requires Databricks or a Spark cluster to be effective.\n",
    "\n",
    "2. Loky (joblib.parallel_backend(\"loky\", n_jobs=-1))\n",
    "- Best for single-node, multi-core parallelization (local CPU-based execution)\n",
    "- Uses Python’s Loky multiprocessing backend to distribute trials across multiple CPU cores on a single machine.\n",
    "- Ideal for smaller-scale tuning tasks where Spark overhead isn’t justified.\n",
    "- May run into memory bottlenecks if too many parallel trials are executed on a machine with limited RAM/CPU.\n",
    "- Does not leverage distributed computing beyond a single node.\n",
    "\n",
    "**When to Use Each?**\n",
    "Use joblibspark on Databricks when working with large datasets, long-running experiments, or when leveraging a Spark cluster for hyperparameter tuning. Use loky for local tuning on a single machine when running experiments on a laptop or a non-distributed environment where Spark isn’t available.\n",
    "\n",
    "If you’re on Databricks with access to a Spark cluster, joblibspark is the clear choice for faster, scalable hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99fdf111-b581-488d-adda-1b441d18611f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Define search space"
    }
   },
   "outputs": [],
   "source": [
    "import optuna\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import joblib\n",
    "\n",
    "#Load dataset from our feature table\n",
    "pdf = df.toPandas()\n",
    "\n",
    "#Select features & target\n",
    "features = [\"temperature\", \"precipitation_transformed\"]\n",
    "target = \"yield_bbl\"\n",
    "\n",
    "#Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(pdf[features], pdf[target], test_size=0.2, random_state=42)\n",
    "\n",
    "#Define objective function\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3),\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 500),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 10),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.05, 1.0)\n",
    "    }\n",
    "    \n",
    "    model = xgb.XGBRegressor(objective=\"reg:squarederror\", **params)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    \n",
    "    #Log trial as a nested MLflow run\n",
    "    with mlflow.start_run(nested=True):\n",
    "        mlflow.log_params(params)\n",
    "        mlflow.log_metric(\"trial_MAE\", mae)\n",
    "    \n",
    "    return mae\n",
    "\n",
    "with mlflow.start_run(run_name=\"XGBoost Optuna with Nested Runs\") as parent_run:\n",
    "\n",
    "    #Set parallelism\n",
    "    study = optuna.create_study(direction=\"minimize\", study_name=\"xgboost_optuna\", sampler=optuna.samplers.TPESampler())\n",
    "    with joblib.parallel_backend(\"loky\", n_jobs=-1):\n",
    "        study.optimize(objective, n_trials=50, timeout=600, n_jobs=4) #Set parallelism to the number of available cores\n",
    "\n",
    "#Get best parameters\n",
    "best_params = study.best_params\n",
    "\n",
    "#Log best parameters to parent run\n",
    "mlflow.log_params(best_params)\n",
    "\n",
    "print(f\"\\n✅ Best Hyperparameters Found: {best_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e0ab5f43-ef7c-414b-bdc5-e25ccfd3607b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Training with the best hyperparameters\n",
    "\n",
    "After a series of runs with Optuna, we've captured and logged the best parameters with our `best_params` object. By using MLFlow's `log_params()` function we can store them as any other object in our experiment for later recall and evaluation.\n",
    "\n",
    "Let's initiate a final training run using just the best parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e769fab-bc3f-42b9-a233-468e3f5e13a3",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Train best parameters for the best version of the model"
    }
   },
   "outputs": [],
   "source": [
    "#Train optimized XGBoost model\n",
    "best_xgb = xgb.XGBRegressor(objective=\"reg:squarederror\", **best_params)\n",
    "best_xgb.fit(X_train, y_train)\n",
    "\n",
    "#Predict and evaluate\n",
    "y_pred = best_xgb.predict(X_test)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "\n",
    "print(f\"\\n✅ Optimized XGBoost Model Performance:\")\n",
    "print(f\"  MAE: {mae:.2f}, RMSE: {rmse:.2f}\")\n",
    "\n",
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "82457ef1-cf8a-4752-8190-ac4165632866",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Logging and registering our model for use\n",
    "Unity Catalog on Databricks is an important tool to help manage ML artifacts and models.\n",
    "\n",
    "Logging and registering your model in Unity Catalog on Databricks ensures centralized governance, versioning, and secure access control across teams and workspaces. By leveraging Unity Catalog, you can:\n",
    "- **Ensure Model Lineage & Reproducibility** – Track model metadata, parameters, and performance metrics alongside datasets and features, ensuring full traceability.\n",
    "- **Enable Cross-Workspace Collaboration** – Share and manage models across multiple Databricks workspaces securely.\n",
    "- **Streamline Deployment & Monitoring** – Seamlessly integrate models with MLflow Serving, batch inference, or real-time applications.\n",
    "- **Enforce Access Controls & Compliance** – Use fine-grained permissions to control who can read, modify, and deploy models, ensuring governance in production.\n",
    "\n",
    "By registering models in Unity Catalog, you create a scalable, production-ready ML lifecycle with governed access, auditing, and operational efficiency, making it ideal for enterprise-wide AI deployments.\n",
    "\n",
    "As long as we set our registry uri to `databricks-uc` MLFlow understands that our uri will contain a 3-level unity namespace. There's a lot to cover in here, which is largely out of scope for this lab, but more information can be found [here](https://docs.databricks.com/aws/en/mlflow/models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33c14a0f-5ecb-413e-8712-b17aaf302ec8",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "set the model registry to global UC"
    }
   },
   "outputs": [],
   "source": [
    "#If we want to use the UC registry rather than the local mlflow registry, set databricks-uc as the registry uri\n",
    "mlflow.set_registry_uri(\"databricks-uc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9596784-1ae9-4336-87c9-97b86aeeb338",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Log the model to mlflow and register in uc"
    }
   },
   "outputs": [],
   "source": [
    "from mlflow.models.signature import infer_signature\n",
    "from mlflow import MlflowClient\n",
    "\n",
    "#Create an instance of the MLFlowClient class\n",
    "client = MlflowClient()\n",
    "\n",
    "#Prepare a sample input for signature inference\n",
    "sample_input = X_train.iloc[:5]  # Take 5 rows from training data\n",
    "sample_output = best_xgb.predict(sample_input)  # Predict output\n",
    "\n",
    "#Infer signature\n",
    "signature = infer_signature(sample_input, sample_output)\n",
    "\n",
    "#Start MLflow run\n",
    "with mlflow.start_run(run_name=\"XGBoost Optuna with joblibspark\") as run:\n",
    "\n",
    "    #Log best hyperparameters\n",
    "    mlflow.log_params(best_params)\n",
    "\n",
    "    #Log model performance\n",
    "    mlflow.log_metric(\"MAE\", mae)\n",
    "    mlflow.log_metric(\"RMSE\", rmse)\n",
    "\n",
    "    #Log model to UC Model Registry\n",
    "    mlflow.xgboost.log_model(\n",
    "        best_xgb,\n",
    "        artifact_path=model_uri,\n",
    "        signature=signature,\n",
    "        input_example=X_test.sample(n=10),\n",
    "        registered_model_name=f\"{catalog}.{db}.{model_name}\"\n",
    "    )\n",
    "\n",
    "    run_id = run.info.run_id\n",
    "    model_path = f'runs:/{run_id}/{catalog}.{db}.{model_name}'\n",
    "    model_version = client.create_model_version(\n",
    "        name=f\"{catalog}.{db}.{model_name}\",\n",
    "        source=model_path,\n",
    "        run_id=run_id\n",
    "    ).version\n",
    "\n",
    "    print(\"✅ Logged optimized model to MLflow\")\n",
    "\n",
    "#Update alias\n",
    "alias_name = \"Champion\"\n",
    "client.set_registered_model_alias(name=f\"{catalog}.{db}.{model_name}\", alias=alias_name, version=model_version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "63448980-35ad-48c0-a41e-241b5ebc6caa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Testing our model\n",
    "Testing our model is as simple as retrieving it from the unity catalog model store and applying it against our original feature table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99dc8af2-1762-4459-97c6-d948badc6dd2",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Sample invocation"
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from pyspark.sql.functions import struct, col\n",
    "logged_model = f'runs:/{run_id}/{catalog}.{db}.{model_name}'\n",
    "\n",
    "#Load model as a Spark UDF. Override result_type if the model does not return double values.\n",
    "loaded_model = mlflow.pyfunc.spark_udf(spark, model_uri=logged_model)\n",
    "\n",
    "#Predict on a Spark DataFrame.\n",
    "df2 = df.withColumn('predictions', loaded_model(struct(*map(col, df.columns))))\n",
    "\n",
    "#Preview our sample forecast\n",
    "display(df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3eca8c53-0edb-43e1-842a-e1e1e7c8e3be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Predictions are expectedly poor - we were tuning this for forward looking forecasting.\n",
    "Also, there are likely better algorithms for this specific dataset since we don't have a lot of valuable ancillary data.\n",
    "\n",
    "This experiment serves as an example for a good starting point. There are many things we can do to iterate and build upon this experiment to improve on it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d399d0eb-e26a-4d34-a3ec-6a3cd5f111f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Lab Challenge:"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "04_MFLow_Experimentation_and_Training",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
