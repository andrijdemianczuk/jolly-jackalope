{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d6bba89-66d8-4be4-9791-1373d7a839a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Oil Extraction Production Forecasting\n",
    "<br/>\n",
    "<img src=\"https://www.nsenergybusiness.com/wp-content/uploads/sites/4/2022/07/refinery-ga56d4972f_640.jpg\" />\n",
    "\n",
    "## Advanced feature engineering\n",
    "In this notebook we'll be taking the information we learned from 01_Data_Exploration and building tranformed feature sets to use in training our model. We will be using Databricks Feature Engineering Client to help us manage and store these values as feature tables and kick off our experiment to log all of our work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed1a1e53-60d7-4f58-b7c9-f55995b7e608",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Initialization\n",
    "Below is an initialization block to help us out. This is designed so that each user has their own set of unique names credentials. Don't worry too much about what it's doing - this is mostly because we have several users doing the same lab with the same parameters in a shared workspace and don't want any collisions. For enterprise work this is largely unnecessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4171ca4-00c8-450b-aa11-2ddd88c40984",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import hashlib, base64\n",
    "\n",
    "#IMPORTANT! DO NOT CHANGE THESE VALUES!!!!\n",
    "catalog = \"workshop\"\n",
    "db = \"default\"\n",
    "current_user = dbutils.notebook.entry_point.getDbutils().notebook().getContext().tags().get(\"user\").get()\n",
    "hash_object = hashlib.sha256(current_user.encode())\n",
    "hash_user_id = base64.b32encode(hash_object.digest()).decode(\"utf-8\").rstrip(\"=\")[:12]  #Trim to 12 chars for readability\n",
    "initials = \"\".join([x[0] for x in current_user.split(\"@\")[0].split(\".\")])\n",
    "short_hash = hashlib.md5(current_user.encode()).hexdigest()[:8]  #Short 8-char hash\n",
    "safe_user_id = f\"{initials.upper()}_{short_hash}\"\n",
    "src_table = f\"{safe_user_id}_oil_yield\"\n",
    "model_name = f\"{safe_user_id}_oil_yield_forecast\"\n",
    "model_uri = f\"{catalog}.{db}.{model_name}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e212aa26-d907-45b7-b02a-2ecab2d763cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Feature Engineering Client\n",
    "The Databricks Feature Engineering Client simplifies the process of creating, managing, and serving features for machine learning models. It provides a unified API to ingest, transform, and store features in a feature table, ensuring consistency between training and inference. With built-in support for feature versioning, lineage tracking, and real-time feature serving, it helps streamline MLOps workflows and reduce data leakage risks. By leveraging Delta Lake and MLflow, the Feature Engineering Client enables efficient, scalable, and reproducible feature engineering in Databricks.\n",
    "\n",
    "In the cell below we will use it to load our features (untransformed) that we created in the last notebook. This allows us to preserve integrity of the features, promote discover of where they are used and manage lineage of our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a2c5ec7-0840-4ab7-a491-cc771962e39f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks.feature_engineering import FeatureEngineeringClient\n",
    "\n",
    "fe = FeatureEngineeringClient()\n",
    "\n",
    "df = fe.read_table(\n",
    "  name=f'{catalog}.{db}.{src_table}_features'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e8c0880-1be4-4567-98b9-c5b47c0e5876",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Where do we normalize & transform our data?\n",
    "We can handle our data normalization in one of two ways. We can either compute the data as it lands in the feature tables which we would normally do as part of the ingestion pipeline or we can late-stage process them as a wrapper function for the compiled model. There are benefits and drawbacks of both, but for this lab we'll be simulating pre-processing the features as though they were part of the ingestion pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b5c6daf-36ab-4183-b6a2-ca180289e7b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Figuring out our lambda values\n",
    "Calculating and storing our lambda values once is a good idea.\n",
    "\n",
    "Storing lambda values for Box-Cox transformations in a dedicated Delta table ensures consistency, reproducibility, and efficiency across our ML pipeline. Since the Box-Cox transformation is parametric, meaning it depends on the estimated lambda to stabilize variance and normalize data, computing these values once prevents discrepancies between training and inference. By storing them in a Delta table, you:\n",
    "- Ensure Consistency – The same lambda values are used across different pipeline stages, avoiding mismatches that could degrade model performance.\n",
    "- Improve Reproducibility – Enables easy retrieval of precomputed lambda values for future transformations, keeping experiments and deployments aligned.\n",
    "- Enhance Efficiency – Avoids redundant recomputation, reducing computational overhead when processing large datasets in Databricks.\n",
    "- Enable Auditing & Versioning – Delta Lake’s built-in version control allows tracking changes to lambda values, making it easier to debug and maintain transformations over time.\n",
    "\n",
    "This structured approach supports scalable and production-ready ML workflows, ensuring that your transformed features remain stable and reliable throughout model development and deployment.\n",
    "\n",
    "In simpler terms, doing it once means it's done the same way every time and can be applied in the future with expected (idempotent) results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c557376b-2da9-4425-92e2-07e21a4dc94d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType\n",
    "import pandas as pd\n",
    "from scipy.stats import boxcox, yeojohnson\n",
    "\n",
    "#Convert PySpark DF to Pandas for Box-Cox Calculation\n",
    "df_pd = df.select(\"yield_bbl\", \"precipitation\", \"temperature\").toPandas()\n",
    "\n",
    "#Apply Box-Cox transformation & store lambda values\n",
    "df_pd[\"yield_bbl\"], lambda_yield = boxcox(df_pd[\"yield_bbl\"] + 1)  # Shift to avoid zero\n",
    "df_pd[\"precipitation\"], lambda_precip = boxcox(df_pd[\"precipitation\"] + 1)\n",
    "df_pd[\"temperature\"], lambda_temp = yeojohnson(df_pd[\"temperature\"])\n",
    "\n",
    "#Convert back to Spark DF\n",
    "df_transformed = spark.createDataFrame(pd.DataFrame(df_pd))\n",
    "\n",
    "#Define schema for lambda values DataFrame\n",
    "schema = StructType([\n",
    "    StructField(\"feature_name\", StringType(), True),\n",
    "    StructField(\"lambda_value\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "#Convert numpy.float64 to native Python float\n",
    "lambda_yield = float(lambda_yield)\n",
    "lambda_precip = float(lambda_precip)\n",
    "lambda_temp = float(lambda_temp)\n",
    "\n",
    "#Add lambda values as feature metadata\n",
    "df_lambdas = spark.createDataFrame([\n",
    "    (\"lambda_yield\", lambda_yield),\n",
    "    (\"lambda_precipitation\", lambda_precip),\n",
    "    (\"lambda_temp\", lambda_temp)\n",
    "], schema)\n",
    "\n",
    "#Store lambda values in Delta table (feature metadata)\n",
    "df_lambdas.write.mode(\"overwrite\").format(\"delta\").saveAsTable(f\"{catalog}.{db}.{src_table}_lambdas\")\n",
    "\n",
    "print(f\"Stored Box-Cox lambdas: {lambda_yield}, {lambda_precip}, {lambda_temp}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c4c8523-1ba4-4f00-8c0e-675fefec5492",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Applying the lambda values\n",
    "Applying the lambda values is a fairly straightforward process of applying the value of the feature as a function of a scalar value between 0 an 1 and dividing it by the lambda value. We apply this on every row and store the values as new columns with the _transformed suffix to preserve the original values (we'll see later that we'll be using an un-transformed version of temperature in our prediction).\n",
    "\n",
    "As long as we apply this consistently, even new data coming in can be transformed in the same way. New data can be either actual or inferred data to make a prediction decision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3730c5c9-982d-459c-bbfa-1e8212efd9b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, log, when, lit\n",
    "import math\n",
    "\n",
    "#Load lambda values from feature store\n",
    "df_lambdas = spark.read.table(f\"{catalog}.{db}.{src_table}_lambdas\")\n",
    "lambda_dict = {row[\"feature_name\"]: row[\"lambda_value\"] for row in df_lambdas.collect()}\n",
    "\n",
    "lambda_yield = lambda_dict[\"lambda_yield\"]\n",
    "lambda_precip = lambda_dict[\"lambda_precipitation\"]\n",
    "lambda_temp = lambda_dict[\"lambda_temp\"]\n",
    "\n",
    "#Define Box-Cox transformation function in PySpark\n",
    "def boxcox_pyspark(column, lambda_value):\n",
    "    return when(lambda_value == 0, log(col(column) + 1)).otherwise(\n",
    "        ((col(column) + 1) ** lambda_value - 1) / lambda_value\n",
    "    )\n",
    "\n",
    "#Apply transformations in PySpark\n",
    "df = df.withColumn(\"yield_bbl_transformed\", boxcox_pyspark(\"yield_bbl\", lit(lambda_yield)))\n",
    "df = df.withColumn(\"precipitation_transformed\", boxcox_pyspark(\"precipitation\", lit(lambda_precip)))\n",
    "df = df.withColumn(\"temperature_transformed\", boxcox_pyspark(\"temperature\", lit(lambda_temp)))\n",
    "\n",
    "print(\"Box-Cox transformed features saved successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ab42aca-c8c7-4bd0-8b58-3faadc717a9b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Committing our transformed features\n",
    "Much like our raw features we stored in the last notebook, we'll create a new feature table with the transformed data. Using the Feature Engineering Client we can store this information in a new feature table.\n",
    "\n",
    "#### New v. Old feature tables\n",
    "Why not just update the original feature table with the transformed values? This is a valid debate and one that can go either way. Personally, I'm a fan of decoupling responsibilities. The first feature table (sans transformations) can be used for a lot more. This feature table contains transformations that are specific to our use case - therefore, it's more clear to me to have a separate feature table dedicated to this use case. I have to assume later that I'll want to fork my original features. I don't want to have recursive logic mudying those waters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10ce9c22-ecde-4430-ae2d-c687805d110b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks.feature_engineering import FeatureEngineeringClient\n",
    "\n",
    "fe = FeatureEngineeringClient()\n",
    "\n",
    "#Create feature table with `id` as the primary key.\n",
    "customer_feature_table = fe.create_table(\n",
    "  name=f'{catalog}.{db}.{src_table}_features_transformed',\n",
    "  primary_keys=['id', 'date'],\n",
    "  schema=df.schema,\n",
    "  description='oil yield features - transformed',\n",
    "  df = df,\n",
    "  timeseries_columns='date'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed9c1fab-850a-429a-a7ca-b49c1cda9a05",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Adding our lambda values to a new experiment\n",
    "This is the first step to setting up an experiment we will be using for the rest of this project. Logging our lambda values as parameters in the experiment helps us tie back the actual values we used when factoring our transformed data. Remember, transformed and normalized data will be what we train our model on. This allows us to associate our lambda values (stored in a dedicated delta table) to the pipeline required for our model evaluation. Keeping all of this information together using an MLFlow experiment is a precursor to an MLOps pipeline and 'true' agile ML Engineering where we can tune, re-train and modify future versions of our model without risk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b601f3ac-3bcc-410e-b2e0-be741bfd4bfc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "#Set a named experiment\n",
    "mlflow.set_experiment(f\"/Users/{current_user}/Oil Extraction Production Forecasting\")\n",
    "\n",
    "#Start MLflow run\n",
    "with mlflow.start_run(run_name=f\"{src_table} BoxCox Transformation\"):\n",
    "\n",
    "    #Log transformation parameters\n",
    "    mlflow.log_param(\"lambda_yield\", lambda_yield)\n",
    "    mlflow.log_param(\"lambda_precipitation\", lambda_precip)\n",
    "    mlflow.log_param(\"lambda_temp\", lambda_temp)\n",
    "\n",
    "    #Log feature table paths\n",
    "    mlflow.log_param(\"transformed_feature_table\", f\"{catalog}.{db}.{src_table}_features_transformed\")\n",
    "    mlflow.log_param(\"lambda_values_table\", f\"{catalog}.{db}.{src_table}_lambdas\")\n",
    "\n",
    "    print(\"Logged Box-Cox transformation details to MLflow.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c49c471a-5102-4ed0-875a-ac712a86f5e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Lab challenge \n",
    "What would be the best way to handle pre-processing?\n",
    "- What would be the costs & benefits of pre-processing in a pipeline?\n",
    "- What would be the costs & benefits of pre-processing in a model function wrapper?\n",
    "- How could we decouple pre-processing further?"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "02_Advanced_Feature_Engineering",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
