{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1d6bba89-66d8-4be4-9791-1373d7a839a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Oil Extraction Production Forecasting\n",
    "<br/>\n",
    "<img src=\"https://www.nsenergybusiness.com/wp-content/uploads/sites/4/2022/07/refinery-ga56d4972f_640.jpg\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4171ca4-00c8-450b-aa11-2ddd88c40984",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#IMPORTANT! DO NOT CHANGE THESE VALUES!!!!\n",
    "catalog = \"workshop\"\n",
    "db = \"default\"\n",
    "current_user = dbutils.notebook.entry_point.getDbutils().notebook().getContext().tags().get(\"user\").get()\n",
    "\n",
    "#IMPORTANT! THIS NEEDS TO BE UNIQUE FOR EVERY PARTICIPANT!!!!\n",
    "#IMPORTANT! THIS NEEDS TO BE THE NAME OF THE TABLE YOU CREATED FOR THIS LAB!!!!\n",
    "src_table = \"ademianczuk_oil_yield\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a2c5ec7-0840-4ab7-a491-cc771962e39f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks.feature_engineering import FeatureEngineeringClient\n",
    "\n",
    "fe = FeatureEngineeringClient()\n",
    "\n",
    "df = fe.read_table(\n",
    "  name=f'{catalog}.{db}.{src_table}_features'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2e8c0880-1be4-4567-98b9-c5b47c0e5876",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "We can handle our data normalization in one of two ways. We can either compute the data as it lands in the feature tables which we would normally do as part of the ingestion pipeline or we can late-stage process them as a wrapper function for the compiled model. There are benefits and drawbacks of both, but for this lab we'll be simulating pre-processing the features as though they were part of the ingestion pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c557376b-2da9-4425-92e2-07e21a4dc94d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType\n",
    "import pandas as pd\n",
    "from scipy.stats import boxcox, yeojohnson\n",
    "\n",
    "# Convert PySpark DF to Pandas for Box-Cox Calculation\n",
    "df_pd = df.select(\"yield_bbl\", \"precipitation\", \"temperature\").toPandas()\n",
    "\n",
    "# Apply Box-Cox transformation & store lambda values\n",
    "df_pd[\"yield_bbl\"], lambda_yield = boxcox(df_pd[\"yield_bbl\"] + 1)  # Shift to avoid zero\n",
    "df_pd[\"precipitation\"], lambda_precip = boxcox(df_pd[\"precipitation\"] + 1)\n",
    "df_pd[\"temperature\"], lambda_temp = yeojohnson(df_pd[\"temperature\"])\n",
    "\n",
    "# Convert back to Spark DF\n",
    "df_transformed = spark.createDataFrame(pd.DataFrame(df_pd))\n",
    "\n",
    "# Define schema for lambda values DataFrame\n",
    "schema = StructType([\n",
    "    StructField(\"feature_name\", StringType(), True),\n",
    "    StructField(\"lambda_value\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "# Convert numpy.float64 to native Python float\n",
    "lambda_yield = float(lambda_yield)\n",
    "lambda_precip = float(lambda_precip)\n",
    "lambda_temp = float(lambda_temp)\n",
    "\n",
    "# Add lambda values as feature metadata\n",
    "df_lambdas = spark.createDataFrame([\n",
    "    (\"lambda_yield\", lambda_yield),\n",
    "    (\"lambda_precipitation\", lambda_precip),\n",
    "    (\"lambda_temp\", lambda_temp)\n",
    "], schema)\n",
    "\n",
    "# Store lambda values in Delta table (feature metadata)\n",
    "df_lambdas.write.mode(\"overwrite\").format(\"delta\").saveAsTable(f\"{catalog}.{db}.{src_table}_lambdas\")\n",
    "\n",
    "print(f\"Stored Box-Cox lambdas: {lambda_yield}, {lambda_precip}, {lambda_temp}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3730c5c9-982d-459c-bbfa-1e8212efd9b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, log, when, lit\n",
    "import math\n",
    "\n",
    "# Load lambda values from feature store\n",
    "df_lambdas = spark.read.table(f\"{catalog}.{db}.{src_table}_lambdas\")\n",
    "lambda_dict = {row[\"feature_name\"]: row[\"lambda_value\"] for row in df_lambdas.collect()}\n",
    "\n",
    "lambda_yield = lambda_dict[\"lambda_yield\"]\n",
    "lambda_precip = lambda_dict[\"lambda_precipitation\"]\n",
    "lambda_temp = lambda_dict[\"lambda_temp\"]\n",
    "\n",
    "# Define Box-Cox transformation function in PySpark\n",
    "def boxcox_pyspark(column, lambda_value):\n",
    "    return when(lambda_value == 0, log(col(column) + 1)).otherwise(\n",
    "        ((col(column) + 1) ** lambda_value - 1) / lambda_value\n",
    "    )\n",
    "\n",
    "# Apply transformations in PySpark\n",
    "df = df.withColumn(\"yield_bbl_transformed\", boxcox_pyspark(\"yield_bbl\", lit(lambda_yield)))\n",
    "df = df.withColumn(\"precipitation_transformed\", boxcox_pyspark(\"precipitation\", lit(lambda_precip)))\n",
    "df = df.withColumn(\"temperature_transformed\", boxcox_pyspark(\"temperature\", lit(lambda_temp)))\n",
    "\n",
    "print(\"Box-Cox transformed features saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10ce9c22-ecde-4430-ae2d-c687805d110b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks.feature_engineering import FeatureEngineeringClient\n",
    "\n",
    "fe = FeatureEngineeringClient()\n",
    "\n",
    "# Create feature table with `id` as the primary key.\n",
    "customer_feature_table = fe.create_table(\n",
    "  name=f'{catalog}.{db}.{src_table}_features_transformed',\n",
    "  primary_keys=['id', 'date'],\n",
    "  schema=df.schema,\n",
    "  description='oil yield features - transformed',\n",
    "  df = df,\n",
    "  timeseries_columns='date'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b601f3ac-3bcc-410e-b2e0-be741bfd4bfc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "# Set a named experiment\n",
    "mlflow.set_experiment(f\"/Users/{current_user}/Oil Extraction Production Forecasting\")\n",
    "\n",
    "# Start MLflow run\n",
    "with mlflow.start_run(run_name=f\"{src_table} BoxCox Transformation\"):\n",
    "\n",
    "    # Log transformation parameters\n",
    "    mlflow.log_param(\"lambda_yield\", lambda_yield)\n",
    "    mlflow.log_param(\"lambda_precipitation\", lambda_precip)\n",
    "    mlflow.log_param(\"lambda_temp\", lambda_temp)\n",
    "\n",
    "    # Log feature table paths\n",
    "    mlflow.log_param(\"transformed_feature_table\", f\"{catalog}.{db}.{src_table}_features_transformed\")\n",
    "    mlflow.log_param(\"lambda_values_table\", f\"{catalog}.{db}.{src_table}_lambdas\")\n",
    "\n",
    "    print(\"Logged Box-Cox transformation details to MLflow.\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "02_Advanced_Feature_Engineering",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
