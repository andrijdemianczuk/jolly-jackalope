{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a2e4015-e240-4f5e-ae1c-456cfb6d3692",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Oil Extraction Production Forecasting\n",
    "<br/>\n",
    "<img src=\"https://www.nsenergybusiness.com/wp-content/uploads/sites/4/2022/07/refinery-ga56d4972f_640.jpg\" />\n",
    "<br/>\n",
    "\n",
    "## Introduction: Forecasting Oil Yield with Databricks, MLflow, and XGBoost\n",
    "\n",
    "In this hands-on workshop, we will explore how to leverage Databricks, MLflow, and XGBoost to forecast oil yield (measured in barrels) based on environmental and operational features. Accurate yield predictions are critical in the energy sector, helping optimize production planning, reduce operational risks, and improve overall efficiency. By applying machine learning techniques, we can uncover complex relationships between geological, weather, and drilling parameters that traditional models may struggle to capture.\n",
    "\n",
    "Throughout the session, we will guide you through the end-to-end workflow—from data ingestion and feature engineering to model training, hyperparameter tuning, and tracking experiments with MLflow. You will learn best practices for working with structured time-series data in a distributed environment, handling missing values, and selecting key features that drive predictive accuracy. By the end of this lab, you will have a practical understanding of how to deploy and scale predictive models for oil yield forecasting using XGBoost on Databricks—enabling data-driven decision-making for more efficient and sustainable operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3bb04ff2-77cc-4a47-a6af-78f1ddbe1759",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Source Data\n",
    "For this lab, we generated some data for you. We've uploaded the .csv data to a dbfs volume. Volumes are logical storage areas for good stuff like source data, images etc. Generally speaking, the structure of data organization is a 3-tier model:\n",
    "1. **Catalog:** This is generally used to organize a series of data. We recommend that business units or project teams share a catalog.\n",
    "1. **Database (also called schema):** Databases are collections of artifiacts, typically with related data. Databases generally contain a couple of sub-types of objects such as: *Tables, Volumes, Models and Functions*\n",
    "1. **Objects:** These are the things that a database or schema can hold\n",
    "- **Tables:** Contain all of our structured data\n",
    "- **Volumes:** Used to store raw data. Generally mapped to some type of cloud storage location. Normally this is where we land data from outside systems.\n",
    "- **Models:** We use this location to store models that we build for easy tracking in production. Many people and applications can consume them from here.\n",
    "- **Functions:** This is a new thing, but advanced functions for transforming data can be stored here. We're not using this for our lab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25b863a3-542b-4e4d-838c-c7b458743c3f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Initialization\n",
    "Below is an initialization block to help us out. This is designed so that each user has their own set of unique names credentials. Don't worry too much about what it's doing - this is mostly because we have several users doing the same lab with the same parameters in a shared workspace and don't want any collisions. For enterprise work this is largely unnecessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9705113-2d82-4e6e-844e-c980887f74cc",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Initialization"
    }
   },
   "outputs": [],
   "source": [
    "import hashlib, base64\n",
    "\n",
    "#IMPORTANT! DO NOT CHANGE THESE VALUES!!!!\n",
    "catalog = \"workshop\"\n",
    "db = \"default\"\n",
    "current_user = dbutils.notebook.entry_point.getDbutils().notebook().getContext().tags().get(\"user\").get()\n",
    "hash_object = hashlib.sha256(current_user.encode())\n",
    "hash_user_id = base64.b32encode(hash_object.digest()).decode(\"utf-8\").rstrip(\"=\")[:12]  #Trim to 12 chars for readability\n",
    "initials = \"\".join([x[0] for x in current_user.split(\"@\")[0].split(\".\")])\n",
    "short_hash = hashlib.md5(current_user.encode()).hexdigest()[:8]  #Short 8-char hash\n",
    "safe_user_id = f\"{initials.upper()}_{short_hash}\"\n",
    "src_table = f\"{safe_user_id}_oil_yield\"\n",
    "model_name = f\"{safe_user_id}_oil_yield_forecast\"\n",
    "model_uri = f\"{catalog}.{db}.{model_name}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b5c1b5a-5022-4e39-83bd-4862f423bb8a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Creating the source data table\n",
    "There are many ways to ingest data. In a production case, we'd build some type of ingestion pipeline. This gives us the flexibility to ingest data in-flight or at-rest at a variety of different points.\n",
    "\n",
    "### Using Databricks Volumes\n",
    "The simplest and easiest way to ingest data is using Databricks Volumes. These are mount points within the context of a database. Data can either be dragged and dropped there using the Databricks Web UI, or added by an external application. This will be the starting point of our data journey. For this lab, we've included a filed called `data/synthetic_oil_yield_csv_files/synthetic_oil_yield_20250227_181101.csv`. We've uploaded this to the workshop catalog, under the default database. Let's go ahead and create our first table with this data.\n",
    "\n",
    "#### 1. Create a unique id\n",
    "First, we'll create a unique id to use so we don't run into any collisions with other users. Normally in an organization we'd decide on a unique name or naming convention. Run the code cell below to generate your unique id.\n",
    "\n",
    "#### 2. Copy the unique id\n",
    "Next, copy your unique id. It should be something like `AC_5fae8e4d_oil_yield`. Store this unique code somewhere safe so we can re-use it in a minute.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b94ad2b-fd20-4570-9af2-78ba2156fa89",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#THIS IS UNIQUE FOR EVERY USER\n",
    "#Use this value and create a new delta table under workshop.default with this table!\n",
    "print(src_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f01a163a-0edb-4fc3-a9a4-da32c6aece83",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 3. Find the storage volume\n",
    "Using the Catalog explorer, navigate to the `workshop` catalog, under the `default` database. There you should see a group called `volumes` with a volume called `data`. `data` is where we uploaded the data file. If it does not exist (you are running this in your own environment for example) you can create your own storage volume and upload the file there. More info on how to do that can be found [here](https://docs.databricks.com/aws/en/volumes).\n",
    "#### 4. Create the table from the volume\n",
    "Select the three vertical dots to the right of the file and select `create table`.\n",
    "<img src=\"https://raw.githubusercontent.com/andrijdemianczuk/jolly-jackalope/refs/heads/main/src/Screenshot%202025-03-02%20at%2022.14.52.png\" width=\"750\" />\n",
    "#### 5. Configure the table\n",
    "In the dialog that comes up, set the following parameters:\n",
    "- Catalog: `workshop`\n",
    "- Database: `default`\n",
    "- Table Name: `{Your Unique ID from above}`\n",
    "<img src=\"https://raw.githubusercontent.com/andrijdemianczuk/jolly-jackalope/refs/heads/main/src/Screenshot%202025-03-02%20at%2022.15.15.png\" width=\"750\" />\n",
    "#### 6. Create the table\n",
    "Click the `Create table` button and wait a few minutes. You just created your first delta table!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9125b724-1770-4324-80ea-2581b204d531",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Using PySpark dataframes\n",
    "Next we'll load our delta table we just created into a pyspark dataframe. Most of our work will be done both in pyspark dataframes and pandas dataframes. The data formats are largely interchangeable.\n",
    "\n",
    "### A bit about PySpark dataframes\n",
    "PySpark DataFrames are distributed, schema-aware data structures optimized for big data processing in Apache Spark, making them a powerful choice for handling large-scale datasets in Databricks. Unlike traditional Pandas DataFrames, PySpark DataFrames distribute computations across multiple nodes, enabling efficient parallel processing and scalability. They integrate seamlessly with Delta Lake, MLflow, and Databricks’ managed compute environment, allowing users to perform ETL, feature engineering, and machine learning workflows at scale. With built-in support for SQL queries, transformations, and optimizations via Catalyst and Tungsten, PySpark DataFrames provide a flexible and efficient way to process structured and semi-structured data in Databricks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0283d0f9-78a4-498e-8ba0-231ca22859ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Load the delta table into a PySpark dataframe\n",
    "df = spark.table(f\"{catalog}.{db}.{src_table}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "002f2e77-8c79-4f75-90f0-2632c3701d4d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Let's quickly preview the schema of our dataframe. This helps us understand what fields are available to us and how we interact with them. Previewing the first few rows may also help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e26d668-1f88-4a0a-b34d-a7496e0ff655",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.printSchema()\n",
    "df.show(5, truncate=False)  # Display first 5 rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "010c9dc5-4ddb-4333-b76c-306f9e02e4aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Describing data\n",
    "Running `df.describe()` gives us a quick sense of the shape of our data. Immediately, we can see the brief outline of our numeric features. Most of our fields in this example are numeric with the exception of the well_id and _rescued columns.\n",
    "\n",
    "#### A quick note on _rescued data\n",
    "A _rescued column in Databricks appears when using Auto Loader or schema evolution to ingest data with fields that don’t match the expected schema. Instead of failing the ingestion, Databricks captures these unexpected or unrecognized columns in a single _rescued_data column (in JSON format) to preserve the data and allow for later inspection. This feature helps maintain data integrity and enables debugging schema mismatches without data loss, making it especially useful when working with evolving or semi-structured datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7bec5226-7fee-4500-b4b5-77d302c58e3b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a4d1a6a-a142-4c9b-b7ca-c7c4b0307fbf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Analyzing our data\n",
    "Understanding our data is essential to knowing how to deal with it for our use cases. Missing data is usually the most obvious in need to address. Let's quickly look for the incidence rate of missing data in our dataframe. This can typically be corrected in a number of ways:\n",
    "\n",
    "- **Remove Missing Data** – Drop rows or columns with excessive missing values if they don’t contribute useful information.\n",
    "- **Imputation** – Fill missing values using statistical methods:\n",
    "    1. _Mean/Median/Mode_ for numerical data\n",
    "    1. _Forward/Backward Fill_ for time-series data\n",
    "\t1. _Constant Value_ (e.g., “Unknown” for categorical features)\n",
    "- **Predictive Imputation** – Use machine learning models (e.g., k-NN, regression) to estimate missing values.\n",
    "- **Flagging Missingness** – Create a binary indicator column (is_null) to capture missing data patterns.\n",
    "- **Leverage Domain Knowledge** – Replace missing values with domain-specific estimates when applicable.\n",
    "- Use Databricks Functions – Utilize fillna(), dropna(), replace() in PySpark DataFrames for efficient handling.\n",
    "- **Consider Feature Removal** – If a feature has excessive missing values and imputation isn’t viable, consider dropping it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "035f7bba-9612-404f-8bcb-64ec53516761",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# If any columns have missing values, we need to decide whether to fill, drop, or interpolate them. Sometimes empty or missing values may be valuable though.\n",
    "\n",
    "from pyspark.sql.functions import col, sum\n",
    "\n",
    "df.select([sum(col(c).isNull().cast(\"int\")).alias(c) for c in df.columns]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d9801848-8fb9-4fdb-b78f-950237c07e59",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Previewing seasonality\n",
    "Looking for trends is the easiest way to understand timeseries data. Since we're all bound to the same time structures understand the ebb and flow of our data allows us to consider this when forecasting future data in series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "feb9ebcb-b5e8-4b36-b8b3-a6229a386f89",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "use a timeseries chart to preview seasonality"
    }
   },
   "outputs": [],
   "source": [
    "#Let's look for some seasonality based on the timeseries plot\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "#Convert PySpark DataFrame to Pandas for plotting\n",
    "df_pd = df.select(\"date\", \"yield_bbl\").groupby(\"date\").avg(\"yield_bbl\").orderBy(\"date\").toPandas()\n",
    "\n",
    "#Plot a time series\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(df_pd[\"date\"], df_pd[\"avg(yield_bbl)\"], marker=\"o\", linestyle=\"-\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Average Yield (BBL)\")\n",
    "plt.title(\"Oil Yield Trend Over Time\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9902817-9493-4c42-8017-ffc4293b0e20",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Downsampling data\n",
    "First we're going to downsample our data to get a better view of otherwise overly atomic or noisey data. Downsampling seasonal data is crucial for improving model efficiency, reducing noise, and capturing long-term trends without unnecessary granularity. High-frequency seasonal data can introduce redundancy and volatility, making it harder for machine learning models to generalize patterns effectively. By aggregating data at a lower frequency—such as daily to weekly or hourly to daily—we can smooth out short-term fluctuations while preserving the underlying seasonality. This not only enhances computational efficiency but also prevents models from overfitting to minor variations, leading to more stable and interpretable forecasts in time-series analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e91b84b-aa58-4c4d-ba3c-649f76654e16",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "downsample and show temperature and precipitation"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "#Convert PySpark DataFrame to Pandas\n",
    "df_pd = df.select(\"date\", \"temperature\", \"precipitation\").toPandas()\n",
    "\n",
    "#Convert date to datetime\n",
    "df_pd[\"date\"] = pd.to_datetime(df_pd[\"date\"])\n",
    "\n",
    "#Resample to weekly average to reduce data size\n",
    "df_resampled = df_pd.set_index(\"date\").resample(\"W\").mean().reset_index()\n",
    "\n",
    "#Create figure and axes\n",
    "fig, ax1 = plt.subplots(figsize=(12, 5))\n",
    "\n",
    "#Plot temperature on primary y-axis\n",
    "ax1.plot(df_resampled[\"date\"], df_resampled[\"temperature\"], color=\"red\", marker=\"o\", linestyle=\"-\", label=\"Temperature (°C)\")\n",
    "ax1.set_xlabel(\"Date\")\n",
    "ax1.set_ylabel(\"Temperature (°C)\", color=\"red\")\n",
    "ax1.tick_params(axis=\"y\", labelcolor=\"red\")\n",
    "\n",
    "#Create secondary y-axis for precipitation\n",
    "ax2 = ax1.twinx()\n",
    "ax2.bar(df_resampled[\"date\"], df_resampled[\"precipitation\"], color=\"blue\", alpha=0.5, label=\"Precipitation (mm)\")\n",
    "ax2.set_ylabel(\"Precipitation (mm)\", color=\"blue\")\n",
    "ax2.tick_params(axis=\"y\", labelcolor=\"blue\")\n",
    "\n",
    "#Title and layout\n",
    "plt.title(\"Temperature and Precipitation Over Time (Weekly Avg)\")\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da639193-3c34-45a1-a821-822afe1469b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Previewing outliers\n",
    "Looking for outliers is essential in feature engineering and model performance because outliers can skew statistical measures, distort model predictions, and impact overall data integrity. By detecting, analyzing, and handling outliers appropriately (removing, capping, or transforming them), we ensure that models are trained on reliable and representative data, leading to better predictive performance. Outliers can affect our ability to generalize our models and helps prevent overfitting or 'learning the data, not the relationships'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "16f05ee1-0d16-400d-b4b9-b973b810c9f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#We need to check for abnormally high or low values in oil yield (barrels produced), well pressure and oil price.\n",
    "#Convert to Pandas for visualization\n",
    "df_outliers = df.select([\"yield_bbl\", \"temperature\", \"well_pressure\", \"oil_price\"]).toPandas()\n",
    "\n",
    "#Plot boxplots\n",
    "df_outliers.plot(kind=\"box\", subplots=True, layout=(2, 2), figsize=(10, 8), sharex=False, sharey=False)\n",
    "plt.suptitle(\"Box Plot of Key Features\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12619a14-b49d-481b-a1f3-b2a004ae834e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Understanding feature relationships\n",
    "Feature relationships are important to forecasting models. Often environmental factors have an impact on the effectiveness of being able to predict or forecast numeric or timeseries data. Using a heatmap helps us quickly identify highly correlated fields.\n",
    "\n",
    "A feature correlation heatmap helps identify which features are highly correlated so you can remove redundancy, improve model interpretability, and optimize training performance. For XGBoost, this helps prevent overfitting, while for LSTMs, it ensures the model learns meaningful sequential patterns rather than noise or redundant signals.\n",
    "\n",
    "Using adjacent or ancillary data is useful for boosting predictability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a73c1ff-69d3-411d-82c7-6b9df284cca2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Let's look for some field correlation\n",
    "import seaborn as sns\n",
    "\n",
    "#Convert PySpark DF to Pandas\n",
    "df_corr = df.select([\"yield_bbl\", \"temperature\", \"precipitation\", \"humidity\", \"wind_speed\", \"well_pressure\", \"sand_quality\", \"drilling_efficiency\", \"oil_price\"]).toPandas()\n",
    "\n",
    "#Plot a heatmap\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(df_corr.corr(), annot=True, cmap=\"coolwarm\", fmt=\".2f\", linewidths=0.5)\n",
    "plt.title(\"Feature Correlation Heatmap\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db690934-362e-4958-98e3-41f130dec2ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Skew & Kurtosis\n",
    "\n",
    "**Skewness**\n",
    "\n",
    "Skewness measures the asymmetry of a dataset’s distribution. A highly skewed feature (left or right) can negatively impact models that assume a normal distribution, such as linear regression and neural networks. Skewed features can cause biased weight updates in optimization algorithms and distort relationships in models like XGBoost. Addressing skewness through log, square root, or Box-Cox transformations can improve model stability and predictive performance.\n",
    "\n",
    "**Kurtosis**\n",
    "\n",
    "Kurtosis measures the tailedness of a distribution—how often extreme values (outliers) occur. High kurtosis (leptokurtic) means more outliers, which can destabilize models and lead to overfitting, as the model may focus too much on rare events. Low kurtosis (platykurtic) suggests a lack of significant deviations, which can make a model less sensitive to important rare occurrences. Managing kurtosis through outlier detection and robust feature scaling helps improve model generalization and reliability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d6bce76-97b7-45aa-be13-076c939ac127",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "#Convert PySpark DataFrame to Pandas\n",
    "df_pd = df.select(\"yield_bbl\", \"temperature\", \"precipitation\").toPandas()\n",
    "\n",
    "#Create the KDE plot (bell curve)\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "#Plot yield distribution\n",
    "sns.kdeplot(df_pd[\"yield_bbl\"], label=\"Yield (BBL)\", color=\"red\", linewidth=2)\n",
    "\n",
    "#Labels and title\n",
    "plt.xlabel(\"Value\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.title(\"Bell Curve of Yield\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "#Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b311257-2028-46d7-9b14-a3154f0990d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "#Convert PySpark DataFrame to Pandas\n",
    "df_pd = df.select(\"yield_bbl\", \"temperature\", \"precipitation\").toPandas()\n",
    "\n",
    "#Create the KDE plot (bell curve)\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "#Plot yield distribution\n",
    "sns.kdeplot(df_pd[\"temperature\"], label=\"Temperature\", color=\"green\", linewidth=2)\n",
    "\n",
    "#Labels and title\n",
    "plt.xlabel(\"Value\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.title(\"Bell Curve of temperature\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "#Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ce026c9-0e64-44e8-ba65-8e0349353307",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "#Convert PySpark DataFrame to Pandas\n",
    "df_pd = df.select(\"yield_bbl\", \"temperature\", \"precipitation\").toPandas()\n",
    "\n",
    "#Create the KDE plot (bell curve)\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "#Plot yield distribution\n",
    "sns.kdeplot(df_pd[\"precipitation\"], label=\"Precipitation (mm)\", color=\"blue\", linewidth=2)\n",
    "\n",
    "#Labels and title\n",
    "plt.xlabel(\"Value\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.title(\"Bell Curve of precipitation\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "#Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eae5afa2-8a7d-4a36-a7cc-0c9829cf6a6d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### What next?\n",
    "At this point we know that temperature and precipitation are our biggest contributing features to yield. We'll create a feature table with those values for forecasting.\n",
    "\n",
    "Given that we want to boost our forecast with predicting temperature and precipitation, we'll likely also have to forecast those two features as well (here's a hint as to where we're going!). The predictions of those fields will contribute to the prediction of yield. This means that we'll have to fix the distributions of yield and precipitation. The temperature looks okay since we know that seasonally there's a change between summer and winter with a degree of outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc6bcbb6-9f22-4885-95b2-b1370778d2da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Evaluate Skew & Kurtosis\n",
    "Now that we have a pretty good understanding of our data, what do we do about it? Let's start by getting some numbers to quantify our skew and kurtosis values for our target (`yield_bbl`) and related features (`precipitation` and `temperature`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15d2ab42-77a5-4a2c-ac88-33fdbbe1a113",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "#Convert PySpark DataFrame to Pandas\n",
    "df_pd = df.select(\"date\",\"yield_bbl\", \"precipitation\", \"temperature\").toPandas()\n",
    "\n",
    "#Plot distributions\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "sns.histplot(df_pd[\"yield_bbl\"], kde=True, bins=30, color=\"red\")\n",
    "plt.title(\"Yield Distribution\")\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "sns.histplot(df_pd[\"precipitation\"], kde=True, bins=30, color=\"blue\")\n",
    "plt.title(\"Precipitation Distribution\")\n",
    "\n",
    "plt.subplot(1, 3, 3)  \n",
    "sns.histplot(df_pd[\"temperature\"], kde=True, bins=30, color=\"green\")\n",
    "plt.title(\"Temperature Distribution\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a5f4be0a-82cd-4eb1-9beb-d66762dab1f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from scipy.stats import skew, kurtosis\n",
    "\n",
    "print(f\"Yield Skewness: {skew(df_pd['yield_bbl'])}, Kurtosis: {kurtosis(df_pd['yield_bbl'])}\")\n",
    "print(f\"Precipitation Skewness: {skew(df_pd['precipitation'])}, Kurtosis: {kurtosis(df_pd['precipitation'])}\")\n",
    "print(f\"Temperature Skewness: {skew(df_pd['temperature'])}, Kurtosis: {kurtosis(df_pd['temperature'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60794539-2cb5-44be-8054-d9c02b5295c5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Temperature isn't really that skewed (close to zero) and it's Kurtosis is pretty minimal (far from 3) however it is bi-modal which can have a negative effect on our model's precision. Precipitation is the most abhorant and also has the least amount of impact on yield."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "488fcfb7-8f89-486a-b33a-ac572535eab3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Applying algorithmic transformations\n",
    "We know that some of our features will require transformation. Since yield and precipitation are positive values, we can try and normalize them with a box-cox transformation. Since temperature is both positive and negative we could either shift all values above zero, or try to normalize it with a yeo-johnson transformation.\n",
    "\n",
    "We're going to do our initial investigation to see the effect of a box-cox transformation, however we'll be capturing our features and creating our feature tables pre-transformation. We want to encapsulate this kind of transformation in the model itself so we don't have to tightly couple the transformation with the feature engineering and ingestion process.\n",
    "\n",
    "We will do a sample transformation below to see the effect of the box-cox transformation and its effect on our data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e78e99c-73c4-4b24-a22f-fec0634da4da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Creating lambda values\n",
    "lambda values are the aggregated change required across all values of a field to adjust them but preserve their semantics. Since we're dealing with timeseries data, the order which data appears next to its neighbour matters and has a net effect on the shape and distribution of our data. To make sure this is applied to all values equally, we'll need to persist this data (we'll be doing this in the next notebook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0fe7d085-709b-4648-b0c9-4071af66c2ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from scipy.stats import boxcox, yeojohnson\n",
    "\n",
    "df_pd[\"yield_bbl\"], lambda_yield = boxcox(df_pd[\"yield_bbl\"] + 1)  # Shift to avoid zero\n",
    "df_pd[\"precipitation\"], lambda_precip = boxcox(df_pd[\"precipitation\"] + 1)\n",
    "df_pd[\"temperature\"], lambda_temp = yeojohnson(df_pd[\"temperature\"] + 1)\n",
    "\n",
    "print(f\"Box-Cox Lambda for Yield: {lambda_yield}\")\n",
    "print(f\"Box-Cox Lambda for Precipitation: {lambda_precip}\")\n",
    "print(f\"Yeo-Johnson Lambda for Temperature: {lambda_temp}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "004a20ae-a590-4a7b-a270-5ea7abec9c55",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Normalizing data\n",
    "Normalizing data on a standard scale (usually between 0 and 1 or some other standard scale) helps us better understand and preserve the relationships of our data. This keeps our data on even-keel. Since we don't need between 0 and 1 and want to preserve the relative distance of our data we'll use a StandardScaler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "012bb65f-2af9-48f8-822d-ddafcddf7949",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "#Use MinMaxScaler() if you prefer [0,1] range\n",
    "scaler = StandardScaler()  \n",
    "\n",
    "df_pd[[\"yield_bbl\", \"precipitation\", \"temperature\"]] = scaler.fit_transform(df_pd[[\"yield_bbl\", \"precipitation\", \"temperature\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6e14024-5383-433f-a596-ff462539052f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Let's preview our transformations\n",
    "df_pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f358fc6-19d4-41d3-af07-f64e7cb20bed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Previewing the effect of our transformations\n",
    "Now let's have a look and see if we improved at all with our transformation methods. We can see that we may have made temperature actually worse with our yeo-johnson transformation but improved the shape of precipitation and yield. Since yield is our target feature we won't be using the transformation there. Precipitation still has heavy outliers which can affect our model precision which remains heavily skewed. Let's consider that later - this feature might be introducing more noise, compromising our predictability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a7bf706-c18a-4480-aca4-674b1b68c75e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Plot distributions\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "sns.histplot(df_pd[\"yield_bbl\"], kde=True, bins=30, color=\"red\")\n",
    "plt.title(\"Yield Distribution\")\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "sns.histplot(df_pd[\"precipitation\"], kde=True, bins=30, color=\"blue\")\n",
    "plt.title(\"Precipitation Distribution\")\n",
    "\n",
    "plt.subplot(1, 3, 3)  \n",
    "sns.histplot(df_pd[\"temperature\"], kde=True, bins=30, color=\"green\")\n",
    "plt.title(\"Temperature Distribution\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ceae6462-b349-43da-86b7-ad96ed6f54a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "When we plot out our heatmap, we'll also notice that we had a small improvement of temperature and a slight worsening of precipitation as boosting features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d2aa514d-68ea-4c0c-a6bd-be2ed14763b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Plot the heatmap with the transformed variables\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(df_pd.corr(), annot=True, cmap=\"coolwarm\", fmt=\".2f\", linewidths=0.5)\n",
    "plt.title(\"Feature Correlation Heatmap\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "263e20d5-4005-41d4-aa90-6478a01a050e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Reviewing outliers of our transformed data\n",
    "It looks like we did get rid of our outliers for our target feature. Precision still looks skewed but temperature remains fairly well balanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "feff9e2c-a984-435e-8d5c-8cab4df1ade3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Plot boxplots\n",
    "df_pd.plot(kind=\"box\", subplots=True, layout=(2, 2), figsize=(10, 8), sharex=False, sharey=False)\n",
    "plt.suptitle(\"Box Plot of Key Features\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c910e75a-f269-4151-88cd-755ae2565bab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Let's also have a quick look at the normalized relationhip of temperature and precipitation. Here we see that after the normalization process we have a much closer relationship. This means that while precipitation may not have a direct, close, releationship with yield it may be a good booster when we need to predict temperature in the future based on seasonal timing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c401e1c6-14e3-4394-b891-e9fb5e571505",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Convert date to datetime\n",
    "df_pd[\"date\"] = pd.to_datetime(df_pd[\"date\"])\n",
    "\n",
    "#Resample to weekly average to reduce data size\n",
    "df_resampled = df_pd.set_index(\"date\").resample(\"W\").mean().reset_index()\n",
    "\n",
    "#Create figure and axes\n",
    "fig, ax1 = plt.subplots(figsize=(12, 5))\n",
    "\n",
    "#Plot temperature on primary y-axis\n",
    "ax1.plot(df_resampled[\"date\"], df_resampled[\"temperature\"], color=\"red\", marker=\"o\", linestyle=\"-\", label=\"Temperature (°C)\")\n",
    "ax1.set_xlabel(\"Date\")\n",
    "ax1.set_ylabel(\"Temperature (°C)\", color=\"red\")\n",
    "ax1.tick_params(axis=\"y\", labelcolor=\"red\")\n",
    "\n",
    "#Create secondary y-axis for precipitation\n",
    "ax2 = ax1.twinx()\n",
    "ax2.bar(df_resampled[\"date\"], df_resampled[\"precipitation\"], color=\"blue\", alpha=0.5, label=\"Precipitation (mm)\")\n",
    "ax2.set_ylabel(\"Precipitation (mm)\", color=\"blue\")\n",
    "ax2.tick_params(axis=\"y\", labelcolor=\"blue\")\n",
    "\n",
    "#Title and layout\n",
    "plt.title(\"Temperature and Precipitation Over Time (Weekly Avg)\")\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0bcd65c0-2cf1-4dac-9539-13b6cf9e5a9c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Based on the downsampled, transformed, and normalized data, I'm pretty satisfied with temperature and precipitation as _potentially_ relevant features to predict yield. We'll go back to our 'raw' state and create a feature table based on those two numeric fields along our date. Let's keep in mind that although precipitation is still correlated, it's correlation value is still quite low which may introduce more noise than value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "491583ba-630d-48fe-bac6-4503b2fc40c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "\n",
    "#Load our desired features, along with a monotonically increasing id. Feature tables require a unique identifier.\n",
    "df_features = df.select(\"date\", \"temperature\", \"precipitation\", \"yield_bbl\").withColumn(\"id\", monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "296fa914-3238-45e6-b0ca-af6a30e059fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Creating the feature table\n",
    "Now let's create our feature table. We'll be using this feature table for our experiment and training our model. Creating a feature table ensures discoverability and consistency when using this data for modelling and training.\n",
    "\n",
    "Feature tables in Databricks provide a centralized, governed, and scalable way to store and serve features for machine learning models. By leveraging Delta Lake and MLflow, feature tables ensure consistent, high-quality features across training and inference, reducing data drift and improving model reliability. They support real-time and batch feature serving, enable feature reuse across multiple models, and integrate seamlessly with Databricks Feature Store, making it easier to manage feature lineage, versioning, and operationalization at scale.\n",
    "\n",
    "We will source our data in the next notebooks from our newly generated feature table, creating a lineage of events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d70ab4e7-2a20-4373-b4fd-3bbd7f8c1e6b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks.feature_engineering import FeatureEngineeringClient\n",
    "\n",
    "fe = FeatureEngineeringClient()\n",
    "\n",
    "#Create feature table with `id` as the primary key.\n",
    "customer_feature_table = fe.create_table(\n",
    "  name=f'{catalog}.{db}.{src_table}_features',\n",
    "  primary_keys=['id', 'date'],\n",
    "  schema=df_features.schema,\n",
    "  description='oil yield features',\n",
    "  df = df_features,\n",
    "  timeseries_columns='date'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d1badded-73cf-4018-a1fd-d15bc41ee991",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Lab challenge\n",
    "We've largely been focusing on distribution of data.\n",
    "- What other types of exploratory data analysis (eda) would be useful?\n",
    "- What other metrics or values should we consider at this investigation stage?\n",
    "- How could we further normalize the data for better standardization?"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "01_Data_Exploration",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
