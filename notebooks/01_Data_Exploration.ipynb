{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a2e4015-e240-4f5e-ae1c-456cfb6d3692",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Oil Extraction Production Forecasting\n",
    "<br/>\n",
    "<img src=\"https://www.nsenergybusiness.com/wp-content/uploads/sites/4/2022/07/refinery-ga56d4972f_640.jpg\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9705113-2d82-4e6e-844e-c980887f74cc",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Initialization"
    }
   },
   "outputs": [],
   "source": [
    "#IMPORTANT! DO NOT CHANGE THESE TWO VALUES!!!!\n",
    "catalog = \"workshop\"\n",
    "db = \"default\"\n",
    "\n",
    "#IMPORTANT! THIS NEEDS TO BE UNIQUE FOR EVERY PARTICIPANT!!!!\n",
    "#IMPORTANT! THIS NEEDS TO BE THE NAME OF THE TABLE YOU CREATED FOR THIS LAB!!!!\n",
    "src_table = \"ademianczuk_oil_yield\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0283d0f9-78a4-498e-8ba0-231ca22859ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Load the delta table into a PySpark dataframe\n",
    "df = spark.table(f\"{catalog}.{db}.{src_table}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e26d668-1f88-4a0a-b34d-a7496e0ff655",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.printSchema()\n",
    "df.show(5, truncate=False)  # Display first 5 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7bec5226-7fee-4500-b4b5-77d302c58e3b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "035f7bba-9612-404f-8bcb-64ec53516761",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# If any columns have missing values, we need to decide whether to fill, drop, or interpolate them. Sometimes empty or missing values may be valuable though.\n",
    "\n",
    "from pyspark.sql.functions import col, sum\n",
    "\n",
    "df.select([sum(col(c).isNull().cast(\"int\")).alias(c) for c in df.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "feb9ebcb-b5e8-4b36-b8b3-a6229a386f89",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "use a timeseries chart to preview seasonality"
    }
   },
   "outputs": [],
   "source": [
    "#Let's look for some seasonality based on the timeseries plot\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Convert PySpark DataFrame to Pandas for plotting\n",
    "df_pd = df.select(\"date\", \"yield_bbl\").groupby(\"date\").avg(\"yield_bbl\").orderBy(\"date\").toPandas()\n",
    "\n",
    "# Plot time series\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(df_pd[\"date\"], df_pd[\"avg(yield_bbl)\"], marker=\"o\", linestyle=\"-\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Average Yield (BBL)\")\n",
    "plt.title(\"Oil Yield Trend Over Time\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e91b84b-aa58-4c4d-ba3c-649f76654e16",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "downsample and show temperature and precipitation"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Convert PySpark DataFrame to Pandas\n",
    "df_pd = df.select(\"date\", \"temperature\", \"precipitation\").toPandas()\n",
    "\n",
    "# Convert date to datetime\n",
    "df_pd[\"date\"] = pd.to_datetime(df_pd[\"date\"])\n",
    "\n",
    "# Resample to weekly average to reduce data size\n",
    "df_resampled = df_pd.set_index(\"date\").resample(\"W\").mean().reset_index()\n",
    "\n",
    "# Create figure and axes\n",
    "fig, ax1 = plt.subplots(figsize=(12, 5))\n",
    "\n",
    "# Plot temperature on primary y-axis\n",
    "ax1.plot(df_resampled[\"date\"], df_resampled[\"temperature\"], color=\"red\", marker=\"o\", linestyle=\"-\", label=\"Temperature (°C)\")\n",
    "ax1.set_xlabel(\"Date\")\n",
    "ax1.set_ylabel(\"Temperature (°C)\", color=\"red\")\n",
    "ax1.tick_params(axis=\"y\", labelcolor=\"red\")\n",
    "\n",
    "# Create secondary y-axis for precipitation\n",
    "ax2 = ax1.twinx()\n",
    "ax2.bar(df_resampled[\"date\"], df_resampled[\"precipitation\"], color=\"blue\", alpha=0.5, label=\"Precipitation (mm)\")\n",
    "ax2.set_ylabel(\"Precipitation (mm)\", color=\"blue\")\n",
    "ax2.tick_params(axis=\"y\", labelcolor=\"blue\")\n",
    "\n",
    "# Title and layout\n",
    "plt.title(\"Temperature and Precipitation Over Time (Weekly Avg)\")\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "16f05ee1-0d16-400d-b4b9-b973b810c9f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# We need to check for abnormally high or low values in oil yield (barrels produced), well pressure and oil price.\n",
    "# Convert to Pandas for visualization\n",
    "df_outliers = df.select([\"yield_bbl\", \"temperature\", \"well_pressure\", \"oil_price\"]).toPandas()\n",
    "\n",
    "# Plot boxplots\n",
    "df_outliers.plot(kind=\"box\", subplots=True, layout=(2, 2), figsize=(10, 8), sharex=False, sharey=False)\n",
    "plt.suptitle(\"Box Plot of Key Features\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a73c1ff-69d3-411d-82c7-6b9df284cca2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Let's look for some field correlation\n",
    "import seaborn as sns\n",
    "\n",
    "# Convert PySpark DF to Pandas\n",
    "df_corr = df.select([\"yield_bbl\", \"temperature\", \"precipitation\", \"humidity\", \"wind_speed\", \"well_pressure\", \"sand_quality\", \"drilling_efficiency\", \"oil_price\"]).toPandas()\n",
    "\n",
    "# Plot heatmap\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(df_corr.corr(), annot=True, cmap=\"coolwarm\", fmt=\".2f\", linewidths=0.5)\n",
    "plt.title(\"Feature Correlation Heatmap\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d6bce76-97b7-45aa-be13-076c939ac127",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# Convert PySpark DataFrame to Pandas\n",
    "df_pd = df.select(\"yield_bbl\", \"temperature\", \"precipitation\").toPandas()\n",
    "\n",
    "# Create the KDE plot (bell curve)\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot yield distribution\n",
    "sns.kdeplot(df_pd[\"yield_bbl\"], label=\"Yield (BBL)\", color=\"red\", linewidth=2)\n",
    "\n",
    "# Labels and title\n",
    "plt.xlabel(\"Value\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.title(\"Bell Curve of Yield\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b311257-2028-46d7-9b14-a3154f0990d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# Convert PySpark DataFrame to Pandas\n",
    "df_pd = df.select(\"yield_bbl\", \"temperature\", \"precipitation\").toPandas()\n",
    "\n",
    "# Create the KDE plot (bell curve)\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot yield distribution\n",
    "sns.kdeplot(df_pd[\"temperature\"], label=\"Temperature\", color=\"green\", linewidth=2)\n",
    "\n",
    "# Labels and title\n",
    "plt.xlabel(\"Value\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.title(\"Bell Curve of temperature\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ce026c9-0e64-44e8-ba65-8e0349353307",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# Convert PySpark DataFrame to Pandas\n",
    "df_pd = df.select(\"yield_bbl\", \"temperature\", \"precipitation\").toPandas()\n",
    "\n",
    "# Create the KDE plot (bell curve)\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot yield distribution\n",
    "sns.kdeplot(df_pd[\"precipitation\"], label=\"Precipitation (mm)\", color=\"blue\", linewidth=2)\n",
    "\n",
    "# Labels and title\n",
    "plt.xlabel(\"Value\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.title(\"Bell Curve of precipitation\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eae5afa2-8a7d-4a36-a7cc-0c9829cf6a6d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "At this point we know that temperature and precipitation are our biggest contributing features to yield. We'll create a feature table with those values for forecasting.\n",
    "\n",
    "Given that we want to boost our forecast with predicting temperature and precipitation, we'll likely also have to forecast those two features as well. The predictions of those fields will contribute to the prediction of yield. This means that we'll have to fix the distributions of yield and precipitation. The temperature looks okay since we know that seasonally there's a change between summer and winter with a degree of outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc6bcbb6-9f22-4885-95b2-b1370778d2da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Evaluate Skew & Kurtosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15d2ab42-77a5-4a2c-ac88-33fdbbe1a113",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Convert PySpark DataFrame to Pandas\n",
    "df_pd = df.select(\"date\",\"yield_bbl\", \"precipitation\", \"temperature\").toPandas()\n",
    "\n",
    "# Plot distributions\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "sns.histplot(df_pd[\"yield_bbl\"], kde=True, bins=30, color=\"red\")\n",
    "plt.title(\"Yield Distribution\")\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "sns.histplot(df_pd[\"precipitation\"], kde=True, bins=30, color=\"blue\")\n",
    "plt.title(\"Precipitation Distribution\")\n",
    "\n",
    "plt.subplot(1, 3, 3)  \n",
    "sns.histplot(df_pd[\"temperature\"], kde=True, bins=30, color=\"green\")\n",
    "plt.title(\"Temperature Distribution\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a5f4be0a-82cd-4eb1-9beb-d66762dab1f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from scipy.stats import skew, kurtosis\n",
    "\n",
    "print(f\"Yield Skewness: {skew(df_pd['yield_bbl'])}, Kurtosis: {kurtosis(df_pd['yield_bbl'])}\")\n",
    "print(f\"Precipitation Skewness: {skew(df_pd['precipitation'])}, Kurtosis: {kurtosis(df_pd['precipitation'])}\")\n",
    "print(f\"Temperature Skewness: {skew(df_pd['temperature'])}, Kurtosis: {kurtosis(df_pd['temperature'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60794539-2cb5-44be-8054-d9c02b5295c5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "All three will require a boxcox transformation & normalization on using a standard scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "488fcfb7-8f89-486a-b33a-ac572535eab3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## When to apply transformations\n",
    "Before storing features or after?\n",
    "\n",
    "We're going to do our initial investigation to see the effect of a box-cox transformation, however we'll be capturing our features and creating our feature tables pre-transformation. We want to encapsulate this kind of transformation in the model itself so we don't have to tightly couple the transformation with the feature engineering and ingestion process.\n",
    "\n",
    "We will do a sample transformation below to see the effect of the box-cox transformation and its effect on our data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0fe7d085-709b-4648-b0c9-4071af66c2ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from scipy.stats import boxcox, yeojohnson\n",
    "\n",
    "df_pd[\"yield_bbl\"], lambda_yield = boxcox(df_pd[\"yield_bbl\"] + 1)  # Shift to avoid zero\n",
    "df_pd[\"precipitation\"], lambda_precip = boxcox(df_pd[\"precipitation\"] + 1)\n",
    "df_pd[\"temperature\"], lambda_temp = yeojohnson(df_pd[\"temperature\"] + 1)\n",
    "\n",
    "print(f\"Box-Cox Lambda for Yield: {lambda_yield}\")\n",
    "print(f\"Box-Cox Lambda for Precipitation: {lambda_precip}\")\n",
    "print(f\"Yeo-Johnson Lambda for Temperature: {lambda_temp}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "012bb65f-2af9-48f8-822d-ddafcddf7949",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "scaler = StandardScaler()  # Use MinMaxScaler() if you prefer [0,1] range\n",
    "\n",
    "df_pd[[\"yield_bbl\", \"precipitation\", \"temperature\"]] = scaler.fit_transform(df_pd[[\"yield_bbl\", \"precipitation\", \"temperature\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6e14024-5383-433f-a596-ff462539052f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a7bf706-c18a-4480-aca4-674b1b68c75e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Plot distributions\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "sns.histplot(df_pd[\"yield_bbl\"], kde=True, bins=30, color=\"red\")\n",
    "plt.title(\"Yield Distribution\")\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "sns.histplot(df_pd[\"precipitation\"], kde=True, bins=30, color=\"blue\")\n",
    "plt.title(\"Precipitation Distribution\")\n",
    "\n",
    "plt.subplot(1, 3, 3)  \n",
    "sns.histplot(df_pd[\"temperature\"], kde=True, bins=30, color=\"green\")\n",
    "plt.title(\"Temperature Distribution\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d2aa514d-68ea-4c0c-a6bd-be2ed14763b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Plot heatmap\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(df_pd.corr(), annot=True, cmap=\"coolwarm\", fmt=\".2f\", linewidths=0.5)\n",
    "plt.title(\"Feature Correlation Heatmap\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "feff9e2c-a984-435e-8d5c-8cab4df1ade3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Plot boxplots\n",
    "df_pd.plot(kind=\"box\", subplots=True, layout=(2, 2), figsize=(10, 8), sharex=False, sharey=False)\n",
    "plt.suptitle(\"Box Plot of Key Features\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c401e1c6-14e3-4394-b891-e9fb5e571505",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Convert date to datetime\n",
    "df_pd[\"date\"] = pd.to_datetime(df_pd[\"date\"])\n",
    "\n",
    "# Resample to weekly average to reduce data size\n",
    "df_resampled = df_pd.set_index(\"date\").resample(\"W\").mean().reset_index()\n",
    "\n",
    "# Create figure and axes\n",
    "fig, ax1 = plt.subplots(figsize=(12, 5))\n",
    "\n",
    "# Plot temperature on primary y-axis\n",
    "ax1.plot(df_resampled[\"date\"], df_resampled[\"temperature\"], color=\"red\", marker=\"o\", linestyle=\"-\", label=\"Temperature (°C)\")\n",
    "ax1.set_xlabel(\"Date\")\n",
    "ax1.set_ylabel(\"Temperature (°C)\", color=\"red\")\n",
    "ax1.tick_params(axis=\"y\", labelcolor=\"red\")\n",
    "\n",
    "# Create secondary y-axis for precipitation\n",
    "ax2 = ax1.twinx()\n",
    "ax2.bar(df_resampled[\"date\"], df_resampled[\"precipitation\"], color=\"blue\", alpha=0.5, label=\"Precipitation (mm)\")\n",
    "ax2.set_ylabel(\"Precipitation (mm)\", color=\"blue\")\n",
    "ax2.tick_params(axis=\"y\", labelcolor=\"blue\")\n",
    "\n",
    "# Title and layout\n",
    "plt.title(\"Temperature and Precipitation Over Time (Weekly Avg)\")\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0bcd65c0-2cf1-4dac-9539-13b6cf9e5a9c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "So based on this, I'm pretty satisfied with temperature and precipitation as potentially relevant features to predict yield. We'll go back to our 'raw' state and create a feature table based on those two numeric fields along our date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "491583ba-630d-48fe-bac6-4503b2fc40c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "\n",
    "#Load our desired features, along with a monotonically increasing id. Feature tables require a unique identifier.\n",
    "df_features = df.select(\"date\", \"temperature\", \"precipitation\", \"yield_bbl\").withColumn(\"id\", monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "296fa914-3238-45e6-b0ca-af6a30e059fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Now let's create our feature table. We'll be using this feature table for our experiment and training our model. Creating a feature table ensures discoverability and consistency when using this data for modelling and training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d70ab4e7-2a20-4373-b4fd-3bbd7f8c1e6b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks.feature_engineering import FeatureEngineeringClient\n",
    "\n",
    "fe = FeatureEngineeringClient()\n",
    "\n",
    "# Create feature table with `id` as the primary key.\n",
    "customer_feature_table = fe.create_table(\n",
    "  name=f'{catalog}.{db}.{src_table}_features',\n",
    "  primary_keys=['id', 'date'],\n",
    "  schema=df_features.schema,\n",
    "  description='oil yield features',\n",
    "  df = df_features,\n",
    "  timeseries_columns='date'\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "01_Data_Exploration",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
