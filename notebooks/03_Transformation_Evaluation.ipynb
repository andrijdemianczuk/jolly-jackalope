{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "754dfed1-1df4-445b-a052-ee9c076f66e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Oil Extraction Production Forecasting\n",
    "<br/>\n",
    "<img src=\"https://www.nsenergybusiness.com/wp-content/uploads/sites/4/2022/07/refinery-ga56d4972f_640.jpg\" />\n",
    "\n",
    "## Evaluating our transformations\n",
    "Transformations and normalization are good, only insofar that they have a net-positive impact on our model. In this notebook we'll be doing a sample training run to look at mean-absolute error and root mean-squared error to see what kind of impact our transformations have. This is an important step, and we must consider if we were to 'production-alize' this into a workflow pipeline. Once we have a good idea of the effect our transformations have, we can make an informed decision on how to best train our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4d54767c-465f-4213-a230-076757023894",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "### Initialization\n",
    "Below is an initialization block to help us out. This is designed so that each user has their own set of unique names credentials. Don't worry too much about what it's doing - this is mostly because we have several users doing the same lab with the same parameters in a shared workspace and don't want any collisions. For enterprise work this is largely unnecessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7bf4a4b0-2add-47da-b679-529edfd7edd7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import hashlib, base64\n",
    "\n",
    "#IMPORTANT! DO NOT CHANGE THESE VALUES!!!!\n",
    "catalog = \"workshop\"\n",
    "db = \"default\"\n",
    "current_user = dbutils.notebook.entry_point.getDbutils().notebook().getContext().tags().get(\"user\").get()\n",
    "hash_object = hashlib.sha256(current_user.encode())\n",
    "hash_user_id = base64.b32encode(hash_object.digest()).decode(\"utf-8\").rstrip(\"=\")[:12]  #Trim to 12 chars for readability\n",
    "initials = \"\".join([x[0] for x in current_user.split(\"@\")[0].split(\".\")])\n",
    "short_hash = hashlib.md5(current_user.encode()).hexdigest()[:8]  #Short 8-char hash\n",
    "safe_user_id = f\"{initials.upper()}_{short_hash}\"\n",
    "src_table = f\"{safe_user_id}_oil_yield\"\n",
    "model_name = f\"{safe_user_id}_oil_yield_forecast\"\n",
    "model_uri = f\"{catalog}.{db}.{model_name}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fe4ab4de-befb-4643-adf2-da9a63bffbfb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Loading our experiment context\n",
    "Since we've already set up an experiment in the previous notebook, we'll load that one and use it going forward. We want to track everything in a common place to make it easy to tie our results back to our work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7eb48a0c-3ad7-4aba-a74e-e9deb1e75c7b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "#Set a named experiment. We want to use the same experiment where we logged our feature artifacts\n",
    "mlflow.set_experiment(f\"/Users/{current_user}/Oil Extraction Production Forecasting\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "352ee287-4d59-4652-8050-744d3c7856ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Loading our transformed features\n",
    "Just like last time, we'll use the feature engineering client to load our transformed features table. This will preserve the lineage of our work. Since we want equal access to the transformed and pre-transformed data we'll load all features for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b68dbb96-527b-428d-b07b-4b82f8eb63da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks.feature_engineering import FeatureEngineeringClient\n",
    "\n",
    "fe = FeatureEngineeringClient()\n",
    "\n",
    "#Read in our feature table with normalized & tranformed features for model training\n",
    "df = fe.read_table(\n",
    "  name=f'{catalog}.{db}.{src_table}_features_transformed'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c284813b-d20c-4648-aeb4-faa2a25c6c89",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Evaluating and comparing transformations\n",
    "Since we have our full feature set, we can look at the effect of our transformations and compare them.\n",
    "\n",
    "A Box-Cox transformation is a power transformation that stabilizes variance, reduces skewness, and makes data more normally distributed, improving the effectiveness of statistical models and machine learning algorithms. When applied:\n",
    "- Skewness is reduced, making the data more symmetrical and improving assumptions for models that rely on normality.\n",
    "- Kurtosis is adjusted, reducing extreme outliers and making the distribution more Gaussian-like.\n",
    "- Heteroscedasticity (unequal variance) is minimized, improving model stability.\n",
    "\n",
    "By comparing pre- and post-transformation results, we can assess how well the transformation has normalized the distribution and whether it enhances model performance by making statistical properties more suitable for predictive algorithms.\n",
    "\n",
    "Let's have a look at the distributions side-by-side. We will leverage a seaborn analysis to compare the distributions and plot them out using the plot function of matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f91d1c0-a5db-4bcf-a9d9-7395557efcc4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import skew, kurtosis\n",
    "\n",
    "# Load transformed data\n",
    "df_transformed = df.toPandas()\n",
    "\n",
    "# Define function to print skewness & kurtosis\n",
    "def check_distribution(df, feature):\n",
    "    print(f\"\\nFeature: {feature}\")\n",
    "    print(f\"  Skewness: {skew(df[feature]):.2f}\")\n",
    "    print(f\"  Kurtosis: {kurtosis(df[feature]):.2f}\")\n",
    "\n",
    "# Compare original vs. transformed features\n",
    "for feature in [\"yield_bbl\", \"precipitation\", \"temperature\"]:\n",
    "    print(\"\\nðŸ”¹ BEFORE Transformation:\")\n",
    "    check_distribution(df_transformed, feature)\n",
    "    \n",
    "    print(\"\\nâœ… AFTER Transformation:\")\n",
    "    check_distribution(df_transformed, f\"{feature}_transformed\")\n",
    "\n",
    "# Plot distributions\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "\n",
    "for i, feature in enumerate([\"yield_bbl\", \"precipitation\", \"temperature\"]):\n",
    "    sns.histplot(df_transformed[feature], bins=30, kde=True, ax=axes[0, i], color=\"red\")\n",
    "    axes[0, i].set_title(f\"Before Box-Cox: {feature}\")\n",
    "\n",
    "    sns.histplot(df_transformed[f\"{feature}_transformed\"], bins=30, kde=True, ax=axes[1, i], color=\"blue\")\n",
    "    axes[1, i].set_title(f\"After Box-Cox: {feature}\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "362b2664-f006-4a14-9eef-8e0a8e33872d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "We can see that precipitation definitely benefitted from the transformation. We were able to drastically reduce skew and kurtosis. Although there are a high degree of negative values (potentially impacting our model), we can still justify it's use here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff775f0e-7b12-4e9b-af43-bef359e2720e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Performing a trial run\n",
    "Next, we'll do a trial training run. All we're doing here is looking to see how the box-cox or yeo-johnson transforms affect the reliability of the trained model. We want to see what kind of effect our transformation had on more tangible metrics such as mean-absolute error and root mean-squared error. What we're really looking for here is an improvement (reduction) of both. If little or not effect is noticed we can determine that the transformation didn't add much value. What we want to avoid is worsening scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e76238c-7e0d-4ef4-8dfc-ecb1568629fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "#Select features & target\n",
    "features_original = [\"temperature\", \"precipitation\"]\n",
    "features_transformed = [\"temperature_transformed\", \"precipitation_transformed\"]\n",
    "target = \"yield_bbl\"\n",
    "\n",
    "#Load datasets\n",
    "df_transformed = df.toPandas()\n",
    "\n",
    "#Train-test split\n",
    "X_train_orig, X_test_orig, y_train_orig, y_test_orig = train_test_split(df_transformed[features_original], df_transformed[target], test_size=0.2, random_state=42)\n",
    "X_train_trans, X_test_trans, y_train_trans, y_test_trans = train_test_split(df_transformed[features_transformed], df_transformed[target], test_size=0.2, random_state=42)\n",
    "\n",
    "#Train XGBoost models\n",
    "model_orig = xgb.XGBRegressor(objective=\"reg:squarederror\", n_estimators=100, learning_rate=0.1)\n",
    "model_trans = xgb.XGBRegressor(objective=\"reg:squarederror\", n_estimators=100, learning_rate=0.1)\n",
    "\n",
    "model_orig.fit(X_train_orig, y_train_orig)\n",
    "model_trans.fit(X_train_trans, y_train_trans)\n",
    "\n",
    "#Predictions\n",
    "y_pred_orig = model_orig.predict(X_test_orig)\n",
    "y_pred_trans = model_trans.predict(X_test_trans)\n",
    "\n",
    "#Compute Errors\n",
    "mae_orig = mean_absolute_error(y_test_orig, y_pred_orig)\n",
    "rmse_orig = mean_squared_error(y_test_orig, y_pred_orig, squared=False)\n",
    "\n",
    "mae_trans = mean_absolute_error(y_test_trans, y_pred_trans)\n",
    "rmse_trans = mean_squared_error(y_test_trans, y_pred_trans, squared=False)\n",
    "\n",
    "#Print Results\n",
    "print(\"\\nðŸ”¹ Model Performance (Without Box-Cox):\")\n",
    "print(f\"  MAE: {mae_orig:.2f}, RMSE: {rmse_orig:.2f}\")\n",
    "\n",
    "print(\"\\nâœ… Model Performance (With Box-Cox):\")\n",
    "print(f\"  MAE: {mae_trans:.2f}, RMSE: {rmse_trans:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eb5ec4cc-1cd9-49a6-8dce-5aa1f33897f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Analyzing our results\n",
    "Clearly we didn't see much of a positive change when using our transformed temperature and precipitation. Precipitation is interesting because we noticed it was the most positively affected in terms of reducing skewness and kurtosis with the box-cox power transformer, but it also had the least effect on predictability of yield. Temperature which had a much higher correltation was actually made slightly worse with the box-cox transformation. Despite being bi-modal, the temperature feature was the best predictor of yield other than yield seasonality itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8036034a-cf30-4e52-bf38-e2b607d709a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Lab Challenge: How could we further improve MAE and RMSE?\n",
    "- Further adjustments to features?\n",
    "- HP tuning?\n",
    "- Yeo Johnson?\n",
    "- What other algorithms might be better? LSTM for DNN processing?\n",
    "- What's causing noise?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9baa438f-8f98-4e3d-82f1-d0a8ad86b972",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Tuning and managing our experiment\n",
    "MLFlow is key, and using hyperopt or optuna are good for distributed hyperparameter tuning. In the next notebook, we'll be setting up an MLFlow experiment for training and tuning out model."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "03_Transformation_Evaluation",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
