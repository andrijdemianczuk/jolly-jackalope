{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de37b529-f957-4996-bd8e-9df45e6b939a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Oil Extraction Production Forecasting\n",
    "<br/>\n",
    "<img src=\"https://www.nsenergybusiness.com/wp-content/uploads/sites/4/2022/07/refinery-ga56d4972f_640.jpg\" />\n",
    "\n",
    "## Forecasting yield based on projected temperature and precipitation\n",
    "In this notebook we'll be using our Unity Catalog managed model for prediction of new data. We'll forecast temperature and precipitation and apply our model based on those new estimates.\n",
    "\n",
    "**Note:** In this notebook there's an obvious ommision to applying a box-cox transformation to precipitation. Since we're using prophet based on the timestamp from historical records (which have the transformation applied) we can predict the transformed value. In general, as new data arrives (or is predicted) it should run through the same transformation using the stored lambda values we saved in the feature engineering section of this lab (02_Advanced_Feature_Engineering)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b9406ab3-2ed2-4b3c-9c8e-f37ec2a0ef01",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### A quick note on prophet\n",
    "Prophet is an easy to use, timeseries forecasting algorithm. We'll be using it to forecast weather which is much easier than forecasting yield. With the forecasted weather data we'll infer the barrels of oil yield using our trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d48972b-139d-4403-9ea2-a6c078e9e91e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install prophet\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4c7b66f4-50d1-44af-8ba8-c6d2d3534754",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Initialization\n",
    "Below is an initialization block to help us out. This is designed so that each user has their own set of unique names credentials. Don't worry too much about what it's doing - this is mostly because we have several users doing the same lab with the same parameters in a shared workspace and don't want any collisions. For enterprise work this is largely unnecessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8696245-ad14-4174-ad11-2c3ff3b8a123",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import hashlib, base64\n",
    "\n",
    "#IMPORTANT! DO NOT CHANGE THESE VALUES!!!!\n",
    "catalog = \"workshop\"\n",
    "db = \"default\"\n",
    "current_user = dbutils.notebook.entry_point.getDbutils().notebook().getContext().tags().get(\"user\").get()\n",
    "hash_object = hashlib.sha256(current_user.encode())\n",
    "hash_user_id = base64.b32encode(hash_object.digest()).decode(\"utf-8\").rstrip(\"=\")[:12]  #Trim to 12 chars for readability\n",
    "initials = \"\".join([x[0] for x in current_user.split(\"@\")[0].split(\".\")])\n",
    "short_hash = hashlib.md5(current_user.encode()).hexdigest()[:8]  #Short 8-char hash\n",
    "safe_user_id = f\"{initials.upper()}_{short_hash}\"\n",
    "src_table = f\"{safe_user_id}_oil_yield\"\n",
    "model_name = f\"{safe_user_id}_oil_yield_forecast\"\n",
    "model_uri = f\"{catalog}.{db}.{model_name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f0d1f42-671d-4d3d-a830-0956077e0034",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "# Set a named experiment\n",
    "mlflow.set_experiment(f\"/Users/{current_user}/Oil Extraction Production Forecasting\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33d11dae-8b2d-44fd-952a-06ce8971f5bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks.feature_engineering import FeatureEngineeringClient\n",
    "\n",
    "fe = FeatureEngineeringClient()\n",
    "\n",
    "df = fe.read_table(\n",
    "  name=f'{catalog}.{db}.{src_table}_features_transformed'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f62b6393-a52a-449c-9f88-c02297bb02b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#If we want to use the UC registry rather than the local mlflow registry, set databricks-uc as the registry uri\n",
    "mlflow.set_registry_uri(\"databricks-uc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "72653c99-43c8-4421-9fdd-03a77a52e706",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Loading our best model from Unity Catalog\n",
    "Loading our best-performing model is easy with Unity Catalog model aliases. Although MLOps is out of scope for this lab, it's worth noting that the latest model may not be the best performing one. Generally speaking, having a champion-challenger paradigm when training is a good idea. Models only get promoted if they outperform their predecessor. For the sake of this lab, we'll be assuming the latest version of the model is the best performing one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b53bf6d5-05da-48a6-be96-6c18bba9efc3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from mlflow import MlflowClient\n",
    "\n",
    "# Define Unity Catalog Model URI with alias\n",
    "model_alias = \"Champion\"\n",
    "model_uri = f\"models:/{catalog}.{db}.{model_name}@{model_alias}\"\n",
    "\n",
    "# Load the trained model\n",
    "loaded_model = mlflow.xgboost.load_model(model_uri)\n",
    "\n",
    "print(f\"✅ Model Loaded from Unity Catalog. Loaded {model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "02b081b1-6753-4982-a4ea-8cc8387dd567",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Loading our sample window\n",
    "We're going to sub-sample our data for prophet forecasting. Since we want to the last 30 days, we'll organize our data by descending date. This should give us a good estimate of short-term weather forecasting based on historical seasonality. Let's see how this behaves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a8c823e-76a5-4867-9b02-cfd9284c841b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks.feature_engineering import FeatureEngineeringClient\n",
    "from pyspark.sql.functions import col, date_add\n",
    "import pandas as pd\n",
    "\n",
    "fe = FeatureEngineeringClient()\n",
    "\n",
    "df = fe.read_table(\n",
    "  name=f'{catalog}.{db}.{src_table}_features'\n",
    ").orderBy(col(\"date\").desc()).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "83dcf8b3-7c9a-48bc-b90f-4704dc7aa9d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Let's quickly preview our pandas dataframe to see the top and bottom rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b020b189-0fec-4bec-9a58-4e9a4619d5ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "091b8064-b29e-44d6-8ebb-f087e71a0dc6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Basic forecasting using mean\n",
    "Let's make our first attempt at forecasting using a simple mean() calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f05a7294-61d3-473e-b59c-274be3bd399c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "estimate features based on historical seasonality"
    }
   },
   "outputs": [],
   "source": [
    "from databricks.feature_engineering import FeatureEngineeringClient\n",
    "from pyspark.sql.functions import col, date_add\n",
    "import pandas as pd\n",
    "\n",
    "fe = FeatureEngineeringClient()\n",
    "\n",
    "df_latest_features = fe.read_table(\n",
    "  name=f'{catalog}.{db}.{src_table}_features_transformed'\n",
    ").orderBy(col(\"date\").desc()).limit(30).toPandas()\n",
    "\n",
    "#Generate future dates\n",
    "future_dates = pd.date_range(start=df_latest_features[\"date\"].max(), periods=30, freq=\"D\")\n",
    "\n",
    "#Estimate future temperature & precipitation based on past seasonality\n",
    "df_future_features = df_latest_features.copy()\n",
    "df_future_features[\"date\"] = future_dates\n",
    "df_future_features[\"temperature\"] = df_latest_features[\"temperature\"].mean()  #Replace with seasonal estimate\n",
    "df_future_features[\"precipitation_transformed\"] = df_latest_features[\"precipitation_transformed\"].mean()  #Replace with seasonal estimate\n",
    "\n",
    "print(\"✅ Generated Future Feature Data\")\n",
    "print(df_future_features.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "edd42a61-f026-4a70-8f37-e24de457d639",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Previewing our dataset\n",
    "As expected, each row has the same temperature and precipitation_transformed value. Since our prediction is based on these two fields, it probably won't give us a good prediction once we run these values through our forecasting algorithm (in other words, all of the forecasts will be the same)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a5a721d-627a-4e64-aa59-8afaf4e84c50",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_future_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "85779b9b-ef23-4034-8024-3662e641fbf4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Predicting on the forecast dataset\n",
    "Running the predictions is as simple as declaring the two fields we want to use for each row to build the prediction off of, and declaring a new field with the output. Once we have that pandas dataframe with the predicted column we can plot it out in matplotlib again and see what we get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c50e3494-c70a-4292-8976-70cb6a78f7c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Select input features for prediction\n",
    "X_future = df_future_features[[\"temperature\", \"precipitation_transformed\"]]\n",
    "\n",
    "# Run predictions\n",
    "df_future_features[\"predicted_yield\"] = loaded_model.predict(X_future)\n",
    "\n",
    "# Display results\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(df_future_features[\"date\"], df_future_features[\"predicted_yield\"], marker=\"o\", linestyle=\"dashed\", color=\"red\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Predicted Yield (BBL)\")\n",
    "plt.title(\"Predicted Oil Yield for Next 30 Days\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "print(\"✅ Predictions Complete\")\n",
    "print(df_future_features.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a9072be6-ec82-412c-8a87-ad3ebf3299d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Saving our predicted values\n",
    "Saving our predicted data is easy. All we need to do is convert the pandas dataframe back to a PySpark dataframe and write it back to our delta lake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "16717fb6-9a5f-45d6-a83b-7d397a0b8d27",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Convert Pandas DataFrame to Spark DataFrame\n",
    "df_future_spark = spark.createDataFrame(df_future_features)\n",
    "\n",
    "# Save predictions to a Delta Table\n",
    "df_future_spark.write.mode(\"overwrite\").format(\"delta\").saveAsTable(f\"{catalog}.{db}.{src_table}_predictions\")\n",
    "\n",
    "print(\"✅ Saved Predictions to Unity Catalog Feature Store\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "78b32326-ccf3-4120-aded-456cc7e9da1f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Improving our forecast\n",
    "We can do better. Since we know that the predicted temperature and precipitation values were the same for each prediction row, they're not doing anything useful. Since we're ultimately trying to forecast using temperature and precipitation, we can use prophet to 'extend' out those two timeseries, and use those extensions with our tuned XGBoost model for a better yield forecast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e74b5790-f5b8-4a4b-ac67-edb85b4e04da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from prophet import Prophet\n",
    "import pandas as pd\n",
    "\n",
    "#Load historical feature data (this is the same data we transformed in notebook 02_Advanced_Feature_Engineering)\n",
    "df_features = spark.read.table(f\"{catalog}.{db}.{src_table}_features_transformed\").toPandas()\n",
    "\n",
    "#Ensure date format. Dates aren't always parsed properly.\n",
    "df_features[\"date\"] = pd.to_datetime(df_features[\"date\"])\n",
    "\n",
    "#Prepare data for Prophet. Set our target (y) and our date/time series (ds)\n",
    "temp_df = df_features[[\"date\", \"temperature\"]].rename(columns={\"date\": \"ds\", \"temperature\": \"y\"})\n",
    "precip_df = df_features[[\"date\", \"precipitation_transformed\"]].rename(columns={\"date\": \"ds\", \"precipitation_transformed\": \"y\"})\n",
    "\n",
    "#Train Prophet models for temperature & precipitation\n",
    "temp_model = Prophet()\n",
    "temp_model.fit(temp_df)\n",
    "\n",
    "precip_model = Prophet()\n",
    "precip_model.fit(precip_df)\n",
    "\n",
    "#Forecast next 30 days\n",
    "future_dates = temp_model.make_future_dataframe(periods=30)\n",
    "temp_forecast = temp_model.predict(future_dates)\n",
    "precip_forecast = precip_model.predict(future_dates)\n",
    "\n",
    "#Extract predictions\n",
    "df_predicted_env = future_dates.copy()\n",
    "df_predicted_env[\"temperature\"] = temp_forecast[\"yhat\"]\n",
    "df_predicted_env[\"precipitation_transformed\"] = precip_forecast[\"yhat\"]\n",
    "\n",
    "print(\"✅ Forecasted Temperature & Precipitation for Next 30 Days\")\n",
    "print(df_predicted_env.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cb3aa0f8-5962-4e70-b218-68fcc732daea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Using our tuned model on forecasted features\n",
    "Previewing the first five rows looks promising. We see that both temperature and precipitation have a degree of seasonality to them. This means that each row will have a distinct forecast when we apply it to our XGBoost model. Let's go ahead and try it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6cf046f-e443-41b6-ab78-9cdfade70f39",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "#Select input features for prediction\n",
    "X_future = df_predicted_env[[\"temperature\", \"precipitation_transformed\"]]\n",
    "\n",
    "#Run predictions\n",
    "df_predicted_env[\"predicted_yield\"] = loaded_model.predict(X_future)\n",
    "\n",
    "print(\"✅ Oil Yield Predictions for Next 30 Days\")\n",
    "print(df_predicted_env.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f0d47611-f567-40bd-9cde-0159562d54ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Plotting our predictions\n",
    "This looks promising. Now we'll go ahead and plot out the last 30 records in our dataset. Since our max lookahead was defined when asked the prophet algorithm to forecast the upcoming window, we'll use that for our histogram plot of future values. We can sort by date descending and take the top 30 records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8071f394-1d09-4754-84c6-75db70d37493",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "predict_df = df_predicted_env.sort_values(by='ds', ascending=False).head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e3a98e8-d341-40be-bf4c-8fd102fa31a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "predict_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cc7bf79c-b737-4bf2-935c-b25878a1ca6f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Viewing our predicted data\n",
    "Plotting out the histogram we can see the data shunting slightly at first, with increasing frequency as we extend the forecast further in the future. This has a direct implication on the confidence interval of the forecast. Generally predicting shorter windows in the near-term are more accurate than larger windows in the long-term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9670eb59-3d8f-4c23-82cb-02b74b2a4625",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Plot results\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.plot(predict_df[\"ds\"], predict_df[\"predicted_yield\"], marker=\"o\", linestyle=\"dashed\", color=\"red\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Predicted Yield (BBL)\")\n",
    "plt.title(\"Predicted Oil Yield for Next 30 Days (Using Forecasted Features)\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1f1ebe53-6642-4de5-81fb-2f252c376d4b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Saving our predicted values (again)\n",
    "Since our first version of predicted values wasn't great, let's go ahead and overwrite them. as new predictions are made we may want to merge or append new predictions to the current predict dataset. Also, as data backfills, we'll be able to further tune and adjust the model while tracking for prediction drift. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "758e8850-8887-417e-bc58-2f8b5c54e089",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Convert Pandas DataFrame to Spark DataFrame\n",
    "df_future_spark = spark.createDataFrame(df_predicted_env.rename(columns={\"ds\": \"date\"}))\n",
    "\n",
    "# Save predictions to a Delta Table in Unity Catalog\n",
    "df_future_spark.write.mode(\"overwrite\").format(\"delta\").saveAsTable(f\"{catalog}.{db}.{src_table}_predictions\")\n",
    "\n",
    "print(\"✅ Saved Forecasted Oil Yield to Unity Catalog Feature Store\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "49b89eb9-4284-4c59-bb1f-645b4efd561f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Lab challenge\n",
    "\n",
    "How could we improve our forecasting?\n",
    "- Right now, we're doing a global forecast based on _all wells_. What about running a forecast for each well?\n",
    "- Instead of predicting weather (temperature and precipitation) what would be some alternative methods to get forecast data for those values in the near-term?\n",
    "\n",
    "What would be the best way to carry these predictions going forward?\n",
    "- How often should we re-run the prediction?\n",
    "- How should we be treating and updating the predicted v. actual data?\n",
    "- Would we want online or offline inference?\n",
    "- How would model serving benefit us?"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "05_Forecasting_Example",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
